\documentclass[a4paper]{article}
\setlength{\headheight}{1.1\baselineskip}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
% package for including graphics with figure-environment
\usepackage{graphicx}
\usepackage{hyperref}
% colors for hyperlinks
% colored borders (false) colored text (true)
\hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

% package for bibliography
\usepackage[authoryear,round]{natbib}
% package for expectation signs
\usepackage{amsmath,amssymb,mathtools,bm,etoolbox}
%\documentclass[a4paper,11pt]{report} 
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{enumitem} 
\usepackage{amsmath, amsthm, amssymb}
\usepackage{amsmath}
\newcommand{\abs}[1]{ \left\lvert#1\right\rvert} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% package for header
\usepackage[automark]{scrpage2}
\pagestyle{scrheadings}
\ihead[]{Name of students}
\ohead[]{\today}
\cfoot[]{\pagemark} 
\setheadsepline[122mm]{0.3mm}

\begin{document}
	\title{
	%\begin{figure}[!ht]
	%	\flushleft
	%		\includegraphics[width=0.7\textwidth]{logo.eps}
	%\end{figure}
	\vspace{1cm}
	\Huge Here you can insert the title of \\ your seminar paper \\
	}
	
	\vspace{1cm}
	
	% if you are the only author, you might use the following
	% \author{Name of student}	
	
	% Insert here your name and correct mail address
	\author{\Large \href{mailto:first.student@smail.fh-koeln.de}{First Student} \and \Large \href{mailto:second.student@smail.fh-koeln.de}{Second Student}
	\vspace{1cm}}
	
	% name of the course and module
	\date{
	\large Module: Modulename \\ Course: Coursename \\ 
	\vspace{0.8cm}
	\large Lecturer: Name of the lecturer \\
	\vspace{1cm}
	\today
	}

	\maketitle
	\setlength{\parindent}{0pt}

\vspace{2cm}
\begin{abstract}
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

\end{abstract}
	\newpage
	\tableofcontents
	\newpage
	
\section{Introduction} % (fold)
\label{sec:introduction}

Regression function tells the relationship between dependent variable Y and independent variable X. 
Given an arbitrary data sample $\{X_i,Y_i\}_{i=1}^n$, the dependency can be illustrated as
\begin{equation}
	Y_i = m(X_i) + \epsilon_i, i=1,2 ...,n.
\end{equation}
Härdle(1990) defines the concept smoothing as conditional expectation of $Y_i$ on certain value of $X_i$:
\begin{equation}
	E(Y_i|X_i) = m(X_i), i=1,2 ...,n.
\end{equation}
He emphasized that by excluding error term interpretation can be concentrated on the important details of the mean dependence of Y on X.
Straightforwardly there are two approaches to achieve this aim. 
The classical parametric approach prespecifies on the mean curve m a functional form, which is assumed to be fully described by a finite set of parameters.
On the contrary, a nonparametric approach doesn't force observed data to be projected into a fixed parameterization through prespecifed functional form or error distribution.
While both approaches have their own significant limits, semiparametric approach is developed to mainly avoid misspecification issue faced by parametric approach and  inconveniences of fully nonparametric models such as the curse of dimensionality, difficulty of interpretation, and the lack of extrapolation capability.
In this paper, we will focus on one widely used model of semiparametric approach, the semiparametric single index model.

Before providing the definition of this semiparametric single index regression model, we will firstly use the binary choice model to shed some light on the intuition of this model. 
Herein the necessity of introducing a weight function into estimation of parameters will also be illustrated, since heteroskedasticity of error distribution is commonly inevitable in empirical studies. 
Subsequently the generalized definition of semiparametric single index model will be given, with the laid-out structure of this paper.

\vspace{2mm}
Regarding to Sönderbom(2009), if dependent variable $Y$ is binary, it is typically equal to one for all observations in the data for which the event of interest happens and zero for the remaining observations. Providing a random sample of $\{Y,X\}$, the probability that the event of interest happens for certain $X$ turns out to be the conditional expectation of $Y$. A class of binary response model between dependent variable ($Y$) and covariates ($X$) is then considered as follows:

\begin{eqnarray}
    Pr(Y=1|X) = E(Y|X) \\    
    Pr(Y=1|X) = G(X'\beta),
\end{eqnarray}

where $G$ is a function taking on values strictly between 0 and 1: $0<G(z)<1$, for all real numbers $z$. $G$ is usually taken as a cumulative density function (cdf), monotonically increasing in $z$(i.e. $x'\beta$). Record that a variety of parametric binary choice models are founded on different choice of $G$. For example, if $G$ is prespecified as cdf of standard normal distribution, yielding a Probit model, while a logistic distribution yielding a Logit model. 

Model (4) can also be interpreted with a latent variable framework suggested by Sönderbom(2009):
\begin{eqnarray}
	Y^* = \alpha + X'\beta + u, \nonumber \\
	Y = 1\ if\ Y^* > 0, \nonumber \\
	Y = 0\ if\ Y^* \leq 0.
\end{eqnarray}	
In order to further demonstrate the powerful strength of a semiparametric single index model compared to classical parametric ones, here we elaborate more with an example from labor force participation. 

Consider $Y$ representing labor force participation, equal to 1 if the individual participates in the labor market and 0 otherwise. The explanatory variables vector $X$ can contain a series of economic factors that might influence labor participation, such as gender, age, marital status and education. Assuming a linear relationship between labor force participation and potential determinants, the empirical analysis focuses on the estimation of $\beta$.
Parametric methods to estimate $\beta$ require assumptions on the distribution of the error term $u$. A common assumption in the parametric framework is $ u \sim N(0, \sigma^{2})$. With further identification conditions such as $\sigma = 1$, $\beta$ can be jointly identified (see Madalla (1986)) and we can use maximum likelihood to estimate it. Following equation (3) we get:
\begin{equation} 
\begin{split}
E(Y|X=x) & = \sum_{y=0,1} yPr(y|x) = 1 \times Pr(y=1|x) + 0 \times Pr(y=0|x) \\
 & = Pr(y=1|x) = Pr(\alpha + x'\beta + u_i > 0) \\
 & = Pr(u_i > -(\alpha + x'\beta)) = 1 - Pr(u_i \leq -(\alpha + x'\beta)) \\
 & = 1 - \Phi(-(\alpha + x'\beta)) \\
 & = \Phi(\alpha + x'\beta)\ (exploit\ symmetry),
\end{split}
\end{equation}
where $\Phi(\cdot)$ is the cdf of standard normal distribution. Under this specification model (4) is commonly referred to as a Probit:
\[
Pr(Y=1|x) = \Phi(\alpha + x'\beta) \equiv G(\alpha + x'\beta).
\]
Now we take a closer look at the distribution assumption on $u$: $ u \sim N(0, 1)$. We firstly come to the issue of heteroskedasticity. Estimates of $\beta$ will be inconsistent once the variability of $u$ depends on the level of some components in $X$. Intuitively we assume 
\[
	u_i = e_i \cdot \abs{X_{1i}}\ and\ 
	e_i \sim N(0, 1).
\]
Recording the assumption that $X_i$ is i.i.d, we get
\[
	u_i \sim N(0, X_{i1}^2).
\]
Compared to calculation in (6), now we get
\begin{equation} 
\begin{split}
Pr(y=1|x) & = Pr(\alpha + x'\beta + e \cdot \abs{x_{1}} > 0) \\
 & = Pr(e > -\frac{1}{\abs{x_{1}}}(\alpha + x'\beta)) \\
 & = 1 - Pr(e \leq -\frac{1}{\abs{x_{1}}}(\alpha + x'\beta)) \\
 & = \Phi(\alpha \frac{1}{\abs{x_{1}}} + \beta_1 + \frac{1}{\abs{x_{1}}}x_{-1}'\beta_{-1})\ .
\end{split}
\end{equation}

Notice the altered functional form inside cdf of Probit model in (7) in comparison with (6), we can see that estimation of parameters $\beta$ will be inconsistent in presence of heteroskedasticity. This point will be further discussed later on in this paper when a weight function constructed on $X$ is introduced into Ichimura's estimation method on single index model. 

Furthermore, instead of $u$ satisfying a normal distribution, a symmetric logistic distribution will lead to a logit model, and a tobit model can be obtained with $G$ being the identity function and $Y = max (0, G(X'\beta) + \epsilon)$, where $\epsilon$ is an unobserved, normally distributed random variable with mean zero and independent of X. 
Again, different assumption on the distribution of $u$ leads to different functional forms for the conditional probability of $Y = 1$. Therefore, consistent parametric estimation of $E(Y|X=x) = Pr(Y=1|x)$ requires the $correct$ distribution specification of $u$.
% Generally speaking, model (1) contains many widely used parametric models that assume that $g$ is known up to a finite-dimensional parameter. 

\vspace{2mm} 
While $Y$ can be either discrete or continuous and function $G$ in (4) is a variation of function $m$ in (2) to satisfy the binary choice model setting, we can generalize the model above. If the a priori specification on the functional form of $m$ is abandoned and the linear parametric relationship between $X$ and $\beta$ maintained, we then get a semiparametric single index model. To define it formally, we follow Li and Racine (2007) notation. The model:
 
\begin{eqnarray}
Y = g(X'\beta) + \epsilon,  % with i =1,2 ..., n?, iid?
\end{eqnarray}

where

\begin{enumerate}[label=(\roman*)]
		\item $\{x_i,y_i\}$ for i = 1, ..., n is an i.i.d. sample;
		\item Y is the dependent variable, $X\in \mathbb{R}^{q}$ is a vector of explanatory variables, $\beta$ is the q $\times$ 1 vector of unknown parameters; 
	\item $X'\beta$ is a single index because it is a scalar;
	\item $ E(\epsilon|x) = 0 $;
	\item $g: \mathbb{R} \rightarrow \mathbb{R} $ is not known; % dimensions?
\end{enumerate}

is a single index model.
\vspace{2mm}

This model is referred to as a linear single index model by Ichimura (1993). It is semiparametric as the functional form of the linear index is specified, while $g(\cdot)$ is left unspecified and the distribution of $\epsilon$ conditioned on X is not specified except $ E(\epsilon|X) = 0 $. 


Hence, model (8) with unknown $g(\cdot)$ leads to a more flexible version of parametric models while retaining many of its desirable features. As shown above, model (8) avoids the problem of error distribution misspecification. Meanwhile, the model avoids the curse of dimensionality by reducing the p-dimensional predictor to a univariate single-index. 

Härdle, Hall and Ichimura (1993) have proven that under some smoothing degree, the orientation vector $\beta$ in model (8) can achieve the same convergence rate $n^{-\frac{1}{2}}$ as parametric models. Thus, the single index model reaches greater estimation precision than fully nonparametric estimation with multidimensional index and the same precision as a parametric model. Moreover, the assumptions of a single index model are weaker than those of a parametric model and stronger than those of a fully nonparametric model.  \footnote{However, the necessary assumptions for consistent parametric estimation can be relaxed. In particular, the single index model might have weaker assumptions than a fully parameterized model for structural economic models.} 

Our main focus in this paper is to expound this claim by using Ichimura's estimation method. We will first discuss the identification conditions necessary for uniquely determining $\beta$ and $g(\cdot)$ in Section 2. Then in Section 3 Ichimura's solution will be analyzed in detail, while proof for $n^{-\frac{1}{2}}$ efficiency and asymptotic normality of estimates provided. Here it is worth noticing that although estimation of $\beta$ and $g(\cdot)$ is influenced by the choice of smoothing parameter, namely bandwidth $h$, the same bandwidth can be selected for estimating $\beta$ and $g(\cdot)$. In the meantime, a version of nonlinear-least-squared function which depends both on $\beta$ and $h$ can be minimized $simultaneously$ with respect to these variables (Härdle et al. 1993). Back up with these arguments we will not go deep into bandwidth selection while merely mention in Section 5 the one we choose to use. As a comparison to Ichimura's estimation, another method based on Klein and Spady's semiparametric binary choice model will be concisely clarified in Section 4.

Beyond all distinct improvements, semiparametric models have important disadvantage. They may generate multiple local minima as they require optimizing nonlinear objective functions that might be multimodal or nonconvex. \footnote{Noncave in the case of the maximum likelihood estimator.} This problem seems to be exarcebated for increasing sample sizes or number of explanatory variables (Manski, 1975, 1985; Manski and Thompson, 1986; Cosslett, 1983; Ichimura, 1993; Horowitz, 1992; and Klein and Spady, 1993).\footnote{H{\"a}rdle and Stoker (1989) suggest a direct estimation method that does not require solving an optimization problem involving iterative solutions. This method becomes less attractive than Ichimura (1993) for small-sample settings as the curse of dimensionality inherited from the first stage might not disappear on the second stage. } We will come back to this issue in Section 6 and discuss it by means of Monte Carlo simulations.
% and as accurate as a one-dimensional nonparametric mean regression for $g$.

% section introduction (end) 

\section{Identification} % (fold)
\label{sec:Identification}

Model (8) implies $E(Y|x) = g(x'\beta_0)$. Thus Y depends on $x$ only through the linear combination $x'\beta_0$, and this relationship is characterized by the link function $g(\cdot)$. Before estimating  $\beta$ and $ g(\cdot)$, restrictions must be imposed to ensure identification of the semiparametric model

\begin{equation}
E(Y|x) = g(x'\beta_0).
\end{equation}

\newtheorem{prop}{Proposition}[section]

\begin{prop}[Identification of a Single Index Model] 
Identification of $\beta_0$ and $g(\cdot)$ in model (8) requires:
\begin{enumerate}[label=(\roman*)]
\item x should not contain a constant and it must contain at least one continuous variable with nonzero coefficient. Furthermore, one component of $\beta_0$ is set to 1. 
\item The support of $x'\beta_0$ is bounded convex set with at least one interior point. $g$ is differentiable and it is not a constant function on the support of $x'\beta_0$;
\item For the discrete components of $x$, varying the values of the discrete variables will not divide the support of $x'\beta_0$ into disjoint subsets. And $g(\cdot)$ must be nonperiodic to permit the existence of discrete components in $x$.
\end{enumerate}
\end{prop}

\textit{Intuition}. We start with ($i$) and by emphasizing that the elements of \textit{x} cannot suffer from multicollinearity, i.e., $Pr(x'\alpha = c) = 1$ where $\alpha$ is a constant and \textit{c }is a scalar. Further, the requirement that \textit{x} contains at least one continuous variable (with nonzero coefficient) prevents that $x$ as well as the scalar variable $ v = x'\beta_0 $ for any vector $\beta_0$ have a finite support. Then $E(Y|X = x) = g(x'\beta_0)$ would impose only a finite number of restrictions  on $g(\cdot)$, leading to a infinite number of different choices for $g(\cdot)$ and $\beta_0$ that satisfy those restrictions.\footnote{Note however that if $g$ is assumed to be increasing, we can identify bounds on the components of $\beta_0$. See Horowitz (1998) for  concrete examples.} In addition, \textit{location normalization} and \textit{scale normalization} requirements are necessary. Define the function $g^{*}$ by the relation $g^{*}(\gamma + v\delta) = g(v)$ for all \textit{v} in the support of $x'\beta_0$. Then

\begin{equation}
E(Y|X = x) = g(x'\beta_0)
\end{equation}

and

\begin{equation}
E(Y|X = x) = g^*(\gamma + x'\beta_0\delta)
\end{equation}

The models (10) and (11) are observationally equivalent. Thus, $\beta_0$ and $g$ are not identified unless restrictions are imposed to uniquely specify $\gamma$ and $\delta$. The restriction on $\gamma$ is then a \textit{location normalization} and it can be satisfied by for example requiring $x$ not to include a constant. The restriction on $\delta$ is a \textit{scale normalization}. In this case we use the approach that assumes that the vector $\beta_0$ has one of its components set to 1. \footnote{This implies that X must have at least 2 dimensions. Otherwise $\beta_0$ is simply normalized to 1 and a one-dimensional nonparametric model $E(y|x) = g(x)$ with no semiparametric part is obtained instead.}

As to what concerns part ($ii$), $g(\cdot)$ cannot be a constant function, as otherwise $\beta_0$ is not identified. Furthermore, what makes the identification of $E(Y|X = x)$ possible is that it remains constant if $x$ changes in a way such that $x'\beta_0$ stays constant. However $P(x'\beta_0 = c)=0$, for $x_0'\beta$ continuously distributed and for some constant $c$, thus leaving identification impossible. By adding the assumption that $g$ is differentiable, $g(x'\beta_0)$ is close to $g(c)$ whenever $x'\beta_0$ is close enough to $c$. Then, the set of $x$ for which $x'\beta_0$ is within any specified nonzero distance of $c$ has nonzero probability for $c$ in the interior of the support of $x'\beta_0$. Therefore, we identify $\beta_0$ by the approximate constancy of $x'\beta_0$. In fact, Wei Lin and Kulasekera (2007) show that the weaker assumption that $g$ is only continuous is sufficient for identification. However, assuming differentiability will become useful when analyzing Ichimura's and Klein and Spady's methods.

To understand why condition ($iii$) we restate an example given by Horowitz (2009). Suppose a Single Index Model of the form (9), in which X has a continuous component $X_1$ with support $\big[0,1\big]$, and one discrete component $X_2$ whose support is $\{0,1\}$. Further assume $X_1$ and $X_2$ are independent, g is strictly increasing and non periodic and set $\beta_1 = 1$ as a \textit{scale normalization}. Then we get the model:
\begin{equation}
\begin{split}
E[Y| X = (x_1,0)]& = g(x_1),\ support\ of\ g(\cdot): [0,1];  \\
E[Y| X = (x_1,1)]& = g(x_1+\beta_2),\ support\ of\ g(\cdot): [\beta_2,1+\beta_2].
\end{split}
\end{equation}
On support $[0,1]$ the function g could be easily identified. However, difficulties may show up when identifying $\beta_2$. If $\beta_2 > 1$ the support $[\beta_2,1+\beta_2]$ in the second equation is disjoint from $\big[0,1\big]$. On $[\beta_2,1+\beta_2]$ as $\beta_2$ is an intercept in the model for $E(Y|(x_1,1))$ which violates the local normalization restriction. Therefore it is not possible to identify $\beta_2$. To resolve this problem we need a overlap between supports, which requires $\beta_2 < 1$. Since function g is identified on $[0,1]$, it is naturally a known condition on $[\beta_2,1]$, therefore $\beta_2$ can be identified by solving the second equation in model (12). 
Moreover providing $\beta_2 < 1$ is satisfied, we check non-periodicity restriction on functional form of g. Suppose for now that g is periodic, using estimates of $\hat{g}$ on support $[\beta_2,1]$ leads to more than one solution of $(x_1+\beta_2)$, so in the end $\beta_2$ will not be identified.

\section{Ichimura's estimation model} % (fold)
\label{sec:Ichimura's estimation model}

In this section Ichimura's (1993) estimation method for semiparametric models is analyzed. This method exhibits $n^{-\frac{1}{2}}$ efficiency and asymptotic normality. A weighting matrix that reaches the semiparametric efficiency bound is investigated. Nonetheless, multiple local minima may result. Let $\beta_0$ denote the true value of $\beta$. Were $g(\cdot)$ to be known and a nonlinear least squares method could be used to estimate $\beta_0$ by minimizing:

\begin{equation}
S_n(\beta) = \frac{1}{n}\sum_{i = 1}^n\big[Y_i - g(X_i'\beta)\big]^2
\end{equation}

with respect to $\beta$.
In this case as $g(\cdot)$ is not know we need to estimate it. However, this cannot be done directly with kernel estimation as $\beta_0$ is also unknown. Still, for a given $\beta$ we can estimate:

\begin{equation}
G(X_i'\beta) \stackrel{def}{=} E(Yi|X_i'\beta) = E[g(X_i'\beta_0)|X_i'\beta]
\end{equation}
 by the kernel method. Ichimura (1993) proposes modifications of the usual kernel estimation to estimate $G(X_i'\beta)$. First, observation $i$ is excluded from the calculation of $G(X_i'\beta)$. Second, the denominator contain a term $\hat{p}_{-i}(X_i'\beta)$. This is a leave-one-out Nadaraya-Watson (NW) kernel estimator and is given by:

\begin{equation}
\hat{G}_{-i}(X_i'\beta) \equiv \hat{E}_{-i}(Yi|X_i'\beta) = \frac{(nh)^{-1}\sum_{j=1, j \neq i }^{n}  Y_jK(\frac{X_j'\beta - X_i'\beta}{h})}{\hat{p}_{-i}(X_i'\beta)},
\end{equation}

where $\hat{p}_{-i}(X_i'\beta) = \frac{1}{nh}\sum_{j=1,j \neq i}^{n}Y_{j}K(\frac{X_j'\beta - X_i'\beta}{h})$.  The denominator $\hat{p}(X_i'\beta)$ is random and it becomes necessary to trim out small values. Let $p(x'\beta)$ denote the probability density function (PDF) of $X_i'\beta$ and $A_\delta$ and $A_n$ be the sets

\[ A_\delta = \{ x : p(x'\beta) \geq \delta, \text{ for all }  \beta \in \mathcal{B} \}
\]

where $\delta > 0$ is a constant, $\mathcal{B} \in \mathbb{R}^q$, and

\[ A_n = \{ x : \norm{x - x^*} \leq 2h_n \text{ for some } x^* \in A_\delta\}.
\]

Then for $x \in A_\delta$ the denominator does not get too close to zero. The set $A_n$ where $\norm{\cdot}$ is a Euclidean norm, is larger than $A_\delta$ but as $ n \rightarrow \infty $, $h_n \rightarrow 0$, $A_n$ shrinks to $A_\delta$. With this in mind it is possible to choose $\beta$ by minimizing the objective function:

\begin{equation}
S_n(\beta) = \frac{1}{n} \sum_{i=1}^{n}  [Y_i - \hat{G}_{-i}(X_i'\beta)]^2w(x_i)\mathbf{1}{(X_i \in A_n)}
\end{equation}

where $\mathbf{1}{(Xi \in A_n)}$ is a trimming function that ensures that the random denominator is positive with high probability, and $w(x_i)$ is an appropriate nonnegative weighting function for possible heteroskedasticity. 

In what follows, certain conditions are necessary to derive the asymptotic distribution of $\hat{\beta}$. In particular, a second order kernel with bounded support must be used and $ h_n = O(n^{-\frac{1}{5}})$.

\newtheorem{theorem}{Theorem}[section]

\begin{theorem}
According to Ichimura (1993)

%\begin{enumerate}[label=(\roman*)]
	%\item The set $A_\delta$ is compact, and the weight function $w(\cdot)$ is bounded and positive on $A_\delta$. Define the set $D_z = \{ z : z = x'\beta, \beta \in \mathcal{B}$,$ x \in A_\delta\}$. Letting $p(\cdot)$ denote the PDF of $z \in D_z$, $p(\cdot)$ is bounded below by a positive constant for all $z \in D_z$.
	%\item $g(\cdot)$ and $p(\cdot)$ are three times differentiable with respect to $z = x'\beta$. The third derivatives are Lipschitz continuous uniformly over $\mathcal{B}$ for all $z \in D_z$.
	%\item The kernel function is a bounded second order kernel having bounded support, is twice differentiable, and its second derivative is Lipschitz continuous.
	%\item $E|Y^m| < \infty$ for some $m \geq 3$. $cov(Y|x)$ is bounded and bounded away from zero for all $x \in A_\delta$. $ln(h)/[nh^{3 + \frac{3}{m-1}}] \rightarrow 0$ and $nh^8 \rightarrow 0$ as $n \rightarrow \infty $ .
	
%\end{enumerate}

we have

\[ \sqrt{n}(\hat{\beta}_n - \beta_0) \stackrel{d}{\rightarrow} N(0,\Omega_I), \] where $\Omega_I = V^{-1}\Sigma V^{-1}$, and

\[\Sigma = E\{w(X_i)^2\sigma^2(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0)) \times (X_i - E_A(X_i|X_i'\beta_0))'\},\]

with $g_i^{(1)} = [\partial g(v)/\partial v]|_{v = X_i'\beta_0}, E_A(X_i|v) = E(X_i|x_A'\beta_0 = v)$ with $x_A$ having the distribution of $X_i$ conditional on $Xi \in A_\delta$, and

\[ V = E[w(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0))(X_i - E_A(X_i|X_i'\beta_0))'].\]

\end{theorem}

Therefore, $\beta$ converges in pobability to $\beta_0$ at rate $n^{-\frac{1}{2}}$, i.e., the same rate as when $g$ is known. Also, the range range of permissible rate of convergence for $h$ includes $n^{-\frac{1}{5}}$, which is the standard rate in nonparametric density.
A consistent estimator of $\Omega_I$ is given by

\[ \hat{\Omega}_I = \hat{V}^{-1}\hat{\Sigma}\hat{V}^{-1}, \]

where $\hat{V} = n^{-1}\sum_{i} w(X_i)\hat{g}^{(1)}(X_i'\hat{\beta}_n)(X_i - \hat{E}(X_i|X_i'\beta))(X_i - \hat{E}(X_i|X_i'\beta))', \hat{\Sigma} = n^{-1}\sum_{i} w(X_i)^2\hat{\epsilon}_{i}^{2}\hat{g}^{(1)}(X_i'\hat{\beta}_n)(X_i - \hat{E}(X_i|X_i'\beta))', \hat{\epsilon}_i = Y_i - \hat{g}(X_i'\hat{\beta}_n), \hat{g}^{(1)}(X_i'\hat{\beta}_n) \linebreak
= [\partial \hat{g}_{-i}/\partial \beta]|_{\beta=\hat{\beta}_n}$, $\hat{g}_{-i}(X_i'\hat{\beta}_n)$ is defined in (10), $\hat{E}(X_i|X_i'\beta)' = \sum_{j} X_jK((X_i - X_j')'\hat{\beta})/ \sum_{j}K((X_i - X_j)'\hat{\beta}_n).$

For what follows the trimming set $A_\delta$ is ignored and $w(\cdot)$ is set to 1. Furthermore, assume $\beta_n - \beta_0 = O(n^{-\frac{1}{2}})$ and $\hat{\beta}_n - \beta_0 = O_p(n^{-\frac{1}{2}})$. Then, 


%and assume that the minimization over $\beta$ is done over the shrinking set $\mathcal{B}_n = \{ \beta : \norm{\beta - \beta_0} \geq Cn^{-1/2}$ for some constant $ C > 0$. This follows H\"ardle, Hall and Ichimura (1993). With this in mind and $ h \in \{h : C_1n^{-\frac{1}{5}} \geq h \geq C_2n^{-\frac{1}{5}}\}$ for some $C_2 > C_1 > 0$ we can establish a proof for Theorem 1. First,

\begin{align*}
S_{n}(\beta) & = \frac{1}{n}\sum_i \{ Y_i - \hat{G}_{-i}(X_i'\beta)\}^2 = \frac{1}{n}\sum_i\{Y_i - \hat{G}_{-i}(X_i'\beta) +  \hat{G}_{-i}(X_i'\beta_0) \\
			 & - \hat{G}_{-i}(X_i'\beta_0) \}^2 = \frac{1}{n} \sum_i \{Y_i - G(X_i'\beta) + o_p(1) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - \hat{G}_{-i}(X_i'\beta_0) \}^2 = \frac{1}{n}\sum_i \{ Y_i - G(X_i'\beta) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - g(X_i'\beta_0) + o_p(1) \}^2 = \frac{1}{n} \sum_i \{ g(X_i'\beta_0) + \epsilon_i - G(X_i'\beta) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - g(X_i'\beta_0) + o_p(1) \}^2 = \frac{1}{n} \sum_i \{ \epsilon_i + \hat{G}_{-i}(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta] \\
			 & + o_p(1) \}^2 = \frac{1}{n}\sum_i \{ g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta] +  \epsilon_i + o_p(1)\}^2 \\
			 & = \frac{1}{n}\sum_i \{ g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta] +  \epsilon_i\}^2 + o_p(1)
\end{align*}

Using a Taylor expansion:

\begin{align*}
g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\hat{\beta}_n)] & = g(X_i'\beta_0) - g(X_i'\hat{\beta}_n) \\
											 & - g^{(1)}(X_i'\hat{\beta}_n)E[(\beta_0 - \hat{\beta}_n)X_i'|X_i'\hat{\beta}_n] + O_p(n^{-1}) \\
				                              & = g^{(1)}(X_i'\hat{\beta}_n)( X_i - E[X_i'|X_i'\hat{\beta}_n)(\beta_0 - \hat{\beta}_n) + O_p(n^{-1})
\end{align*}


It is known that $O_p(n^{-2}) = n^{-2}O_p(1) = o_p(1)$ and following the same reasoning $O_p(n^{-1}) = o_p(1)$ and $O_p(n^{-\frac{1}{2}}) = o_p(1)$. Also, knowing $E(\epsilon) = 0$, the mean of $\epsilon_i$ is $o_p(1)$. Then, $\frac{1}{n}\sum_i\epsilon_io_p(1) = o_p(1)$. Hence, for $g_{i}^{(1)} = g^{(1)}(X_i'\hat{\beta_n})$ and $v_i = X_i - E[X_i'|X_i'\hat{\beta}]$:

\begin{align*}
S_{n}(\hat{\beta}_n) & = (\beta_0 - \hat{\beta}_n)'\left[\frac{1}{n}\sum_i (g_i^{(1)})^2v_iv_i'\right](\beta_0 - \hat{\beta}_n) \\
             & + 2\frac{1}{n}\sum_i\epsilon_ig_i^{(1)}v_i'(\beta_0 - \hat{\beta}_n) + \frac{1}{n}\sum_i \epsilon_i^2 + o_p(1)
\end{align*}

Minimizing in order to $\hat{\beta}_n$ and ignoring terms independent of $\hat{\beta}_n$ and $o_p(1)$:
\[2\frac{1}{n}(\beta_0 - \hat{\beta}_n)\sum_i(g_{i}^{(1)})^2v_iv_i' - 2\frac{1}{n}\sum_i\epsilon_ig_i^{(1)}v_i' = 0 \]
Then, 
\begin{align*}
\sqrt{n}(\beta_0 - \hat{\beta}_n) & = (\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_iv_i')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i}^{(1)}v_i \\
     					  & = (\frac{1}{n}\sum_i(g_{i0}^{(1)})^2v_{i0}v_{i0}')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i0}^{(1)}v_{i0} + o_p(1).
\end{align*}

For $g_{i0}^{(1)} = g^{(1)}(X_i'\beta_0)$ and $v_{i0} = X_i - E[X_i'|X_i'\beta_0]$. With this in mind, the Central Limit Theorem (CLT) and Law of Large Numbers (LLN) we obtain the result from Theorem 3.1 if we set $w(X_i) = 1$.

It is important to however realize that the minimization of a nonlinear objective function such as equation (9) might be computationally costly. Furthermore, there might be multiple local minima because the objective function is multimodal or nonconvex. Direct estimation methods that do not require optimization of problems involving iterative solutions have been developed to counter this problem, such as in H{\"a}rdle and Stoker (1989).


%Recall when we described identiÖcation that we required the dimension of Xi to be 2 or larger.
%Suppose that Xi is one-dimensional.



%\begin{equation}
%W_0(\bar{\beta} - \beta_0) = V_0 + (s.o.),
%\end{equation}


%\[  W_0 = \sum_{i=1}^{n} [g^{(1)}(X_i'\beta_0)]^2[X_i - E(X_i|X_i'\bet_0][X_i - E(X_i|X_i'\beta_0)]' \]

%and

%\[ V_0 = \sum_{i=1}^{n} u_ig^{(1)}(X_i'\beta_0)[X_i - E(X_i|X_i'\beta_0).\]

%Then, by the standard law of large numbers and a central limit theorem (CLT) argument, we have

%\begin{equation}
%\sqrt{n}(\bar{\beta} - \beta_0) = (W_0/n)^{-1}n^{-\frac{1}{2}}V_0 + o_p(1) \overset{d}{\to} N(0,\Sigma_0),
%\end{equation}

%where $\Sigma_0$ is the same as $\Sigma_I$, except that for the former $W(X_i) = 1$.


\subsection{Weight Function} % (fold)
\label{sub:Weight Function}

The weight function, $w$, is chosen so as to maximize the asymptotic efficiency of the estimator. Within the class of weighted nonlinear least squares (WNLS) estimators, an estimator is asymptotically efficient given the covariance matrix $\Omega$ of its asymptotic distributions and the covariance matrix $\Omega^*$ of the other WNLS estimators, if $\Omega^* - \Omega$ is positive semidefinite. Even though the class of all regular estimators of single index models may include estimators that are not semiparametric WNLS estimators, the definition of asymptotically efficient estimator remains the same. 

%Chamberlain (1987) investigated the case for nonlinear regression model (not necessarily a single index model) in which $g$ is known. 

%The model is $E(Y|X = x) = g(x, \beta_0)$. The variance function, $\sigma^2(x) = E\{[Y - g(x, \beta_0)]^2| X = x\}$, is unknown. Chamberlain (1986) showed that the efficiency bound is 

%\[ \Omega_{NLR} = \left\{E\left[\frac{1}{\sigma^2}\frac{\partial g(X, \beta_0)}{\partial \beta} \frac{\partial g(X, \beta_0)}{\partial \beta'}\right]\right\}^{-1} \]

%for a WNLS estimator of $\beta_0$ with weight function $w(x) = \frac{1}{\sigma^2(x)}$. 
The problem of efficient estimation of $\beta_0$ in a single index model with unknown $g$ is analyzed by Hall and Ichimura (1991) and Newey and Stocker (1993). Under certain regularity conditions, the efficiency bound for the single index model with unknown $g$ and using only data for which $X \in A_{\delta}$ is $\Omega_I$ in Theorem 3.1. with $w(x) = \frac{1}{\sigma^2(x)}$. Then, the efficiency bound is

\begin{equation}
\Omega_{SI} = \left\{ E\left[\frac{1}{\sigma^2(x)}\frac{\partial}{\partial \beta}
 G(X'\beta,\beta)\frac{\partial}{\partial \beta} G(X'\beta,\beta) \right] \right\}^{-1}.
\end{equation}

%\I(X \in A_{\delta})}???

This bound is achieved by the semiparametric WNLS estimator if $\sigma^2(X)$ is known or independent of X. The assumption that only data where  $X \in A_{\delta}$ is used can be relaxed by letting $A_\delta$ grow very slowly as $n$ increases. When $\sigma^2(X)$ is unknown it can be replaced by a consistent estimator, say $s_{n}^{2}(x)$. If $w(X) = \frac{1}{s_{n}^{2}(x)}$ in the semiparametric WNLS estimator, the asymptotic efficiency bound will be reached. Thus, an asymptotic efficient estimator of $\beta_0$ is obtained even when $\sigma^2(X)$ is unknown. This consistent estimator can be obtained with a two-step procedure. First, estimate $\beta_0$ by minimizing function (9) with $w(x) = 1$. We obtain an estimator that is $n^{\frac{1}{2}}$-consistent and asymptotically normal but inefficient. The second step is a nonparametric estimation of the mean of the squared first-step residuals conditional on X. This uses the estimated weight function $\hat{w}_i(x) = \frac{1}{\hat{\sigma}_{i}^{2}}$, where  $\hat{\sigma}_{i}^{2}$ is defined by Robinson (1987) as 
\[\hat{\sigma}_{i}^{2} = \frac{1}{k}\sum_{j=1}^{n} \mathbf{1}{(x_j \in N_k(x_i))}\hat{\epsilon}_{j}^{2} ,\]
%The assumption that $X \in A_{\delta}$ can be replaced by the assumption that $A_{\delta}$ grows very slowly as $n$ increases.
and $\hat{\epsilon}_i = y_i - \hat{g}(x_i'\hat{\beta_0})$, for $\hat{\beta_0}$ obtained on the first step. Further, $N_k(x_i)$ is the set of $k$ observations of $x_j$ closest to $x_i$ in weighted Euclidean norm.


\section{Klein and Spady's Binary Choice Estimator} % (fold)
\label{sec:section_about_references_within_the_document}
In this section we analyze Klein and Spady's (1993) semiparametric binary choice model, used to estimate model (2) when $Y \in \{0,1\}$.   This method exhibits $n^{-\frac{1}{2}}$ efficiency, asymptotic normality and asymptotic efficiency. The model is defined as

\begin{equation}
Y_i =  \mathbf{1}{(X_i'\beta \geq \epsilon_i)},
\end{equation}
where $\epsilon$ is a random disturbance. Similarly to Ichimura, let $g$ denote the distribution of $\epsilon_i$. Then, $G(X_i'\beta) = E(g(X_i'\beta_0)|X_i'\beta)$. Additionaly, assume $\epsilon_i$ is independent of $X_i$. For known $g$, the asymptotically efficient estimator of $\beta_0$ is a maximum likelihood estimator (MLE). The estimated $log-likelihood$ function is

\begin{equation}
\mathcal{L}_n(\beta) = \frac{1}{n}\sum_{i=1}^n \left\{ (1 - Y_i)ln[ 1 - g(X_i'\beta)] + Y_iln[g(X_i'\beta)] \right\},
\end{equation}

where

\begin{enumerate}[label=(\roman*)]
		\item Y is binomial with realizations 1 and 0, X is a vector of exogenous variables and $\beta$ is an unknown parameter vector;
		\item $\{x_i,y_i\}$ for i = 1, ..., n is an i.i.d. sample.
\end{enumerate}

The function $\mathcal{L}(\beta)$ is analogous to $S_n(\beta,g)$. It is clear from equation (12) that restrictions must be imposed such that any estimate of $g$ is kept sufficiently far away from 0 and 1. With this in mind, Klein and Spady (1993) use a given trimming function $\tau$ employed to downweight observations for which the corresponding densities are small. However, Klein and Spady (1993) find that $\tau$ has little effect on the numerical performance of the estimator. Thus, the original trimming function can be replaced by a simplified trimming function $\tau_i = \mathbf{1}{(Xi \in A_\delta)}$, which causes little efficiency loss. The latter will be used in the remainder of this section.

Similarly to Ichimura's method, Klein and Spady suggest replacing $g$ with the leave-one-out NW from equation (8). With this in mind, we obtain instead

\begin{equation}
\mathcal{L}_n(\beta) = \frac{1}{n}\sum_{i=1}^n \tau_{i}\{ (1 - Y_i)ln[ 1 - \hat{G}_{-i}(X_i'\beta)] +  Y_iln[\hat{G}_{-i}(X_i'\beta)]\}.
\end{equation}

In what follows, certain assumptions are required in terms of bandwidth and kernel choice to obtain. In particular, Klein and Spady (1993) use an higher order kernel. For example, by requiring
\[\int z^{2}K(z)dz = 0.\]
Furthermore, the bandwidth must satisfy the rate $ n^{-\frac{1}{6}} < h < n^{-\frac{1}{8}}$. Given these restrictions, we obtain Theorem 4.1.


\begin{theorem}
According to Klein and Spady (1993), $\hat{\beta}_{n}$ is $n^{-\frac{1}{2}}$ and has asymptotic normal distribution given by

\[\sqrt{n}(\hat{\beta}_{n} - \beta_0) \stackrel{d}{\rightarrow} N(0,\Omega_{KS}),
\]
where \[ \Omega_{KS} = \left\{ E\left[\frac{\partial}{\partial \beta}
 G(X_i'\beta)\frac{\partial}{\partial \beta} G(X_i'\beta)'\frac{1}{g(X_i'\beta_0)(1 - g(X_i'\beta_0))} \right]\right\}^{-1} \]
 
and $\Omega_{KS} = \Omega_{SI}$, i.e., the estimator is asymptotically efficient.

\end{theorem}

It now becomes necessary to elaborate on the last stament of Theorem 4.1. For a binary choice model, $Var(Y|X = x) = P(Y = 1|X = x)[1 - P(Y = 1|X = x)] = E[\mathbf{1}{(X_i'\beta_0 \geq \epsilon_i)}]\{1 - E[\mathbf{1}{(X_i'\beta_0 \geq \epsilon_i)}]\} = g(x'\beta_0)[1 - g(x'\beta_0)]$ and $\sigma^2(x)$ depends only on the single index $x'\beta_0$. Assume $\beta_n - \beta_0 = O(n^{-\frac{1}{2}})$, $\hat{\beta}_n - \beta_0 = O_p(n^{-\frac{1}{2}})$ and consider the result shown above
\begin{align*}
g(X_i'\beta_0) - E_A[g(X_i'\beta_0)|X_i'\hat{\beta})] & = g^{(1)}(X_i'\hat{\beta})( X_i - E_A[X_i'|X_i'\hat{\beta}])(\beta_0 - \hat{\beta}) \\
											   & + O_p(n^{-1})
\end{align*}
where $E_A(X_i|v) = E(X_i|x_A'\beta_0 = v)$ with $x_A$ having the distribution of $X_i$. Minimizing in order to $\hat{\beta}$ and ignoring terms independent of $\hat{\beta}$ and $O_p(n^{-1})$:
\begin{align*}
 \frac{\partial}{\partial \beta} G(X_i'\hat{\beta}) & = g^{(1)}(X_i'\hat{\beta})( X_i - E_A[X_i'|X_i'\hat{\beta}]) \\
 										   & = g^{(1)}(X_i'\beta_0)( X_i - E_A[X_i'|X_i'\beta]) + o_p(1).
\end{align*}

By substituting this result in equation (10) and $\Omega_I$ in Theorem 3.1., it can be seen that the covariance matrix of the asymptotic distribution of the semiparametric WNLS estimator of $\beta_0$ is the same whether the estimator of $g$ is weighted or not. Moreover, the asymptotically efficient bound $\Omega_{SI}$ is reached without using the weight function. 

%\[S_n(\beta) = -\frac{1}{2n}[ y - g]'\hat{\Sigma}^{-1}[Y - g], \]

%where $\Sigma$ is the error variance matrix. Consider the model for heteroskedasticty $\sigma_i^2 = E[u_i^2|x_i] = exp(z_i'\gamma_0)$, where $z$ is a specified function of $x$. Then $\Sigma = Diag[exp(z_i'\gamma_0)]$ and $\hat{\Sigma} = Diag[exp(z_i'\hat{\gamma})]$, where $\hat{\gamma}$ can be obtained by nonlinear regression of squares NLS residuals $(y_i - g(x_i,\hat{\beta_{NLS}}))^2$ on $exp(z_i'\gamma_0)$. For diagonal $\Sigma$, $\Sigma^{-1} = Diag\left[\frac{1}{\sigma_i^2}\right]$. Then the objective function for the semiparametric WNLS simplifies to

%\[S_n(\beta) = - \frac{1}{2n}\sum_{i-1}^{n}\frac{(y_i - \hat{G}_{-i}(X_i,\beta))^2}{\hat{\sigma_i^2}}.\]

This can be more easily understood by considering a semiparametric WNLS estimation of $\beta_0$ such as with Ichimura's method. Differentiate the right-hand side of equation (13) with respect to $\beta$. Then, we obtain
\begin{equation}
\frac{1}{n} \sum_{i=1}^n \tau_i \frac{Y_i - \hat{G}_{-i}(X_i'\hat{\beta})}{\hat{G}_{-i}(X_i'\hat{\beta})(1 - \hat{G}_{-i}(X_i'\hat{\beta}))} \frac{\partial\hat{G}_{-i}(X_i'\hat{\beta})}{\partial \hat{\beta}} = 0
\end{equation} 

with probability approaching 1 as $n \rightarrow \infty$. This is the same as the first-order condition for semiparametric WNLS estimation of $\beta_0$ with the weight function

\begin{align*}
w(x) & = \{ \hat{G}_{-i}(x'\hat{\beta})[ 1 - \hat{G}_{-i}(x'\hat{\beta})]\}^{-1} = \{ G(x'\hat{\beta})[ 1 - G(x'\hat{\beta})]\}^{-1} + o_p(1) \\
     & = \{ G(x'\beta_0)[ 1 - G(x'\beta_0)]\}^{-1} + o_p(1) = [Var(Y|X = x)]^{-1} + o_p(1).
\end{align*}


Considering section 3.1., it follows that semiparametric maximum-likelihood estimator of $\beta_0$ in Klein and Spady's model is asymptotically efficient.\footnote{This is only true for "first order efficiency", i.e., when $E(X_i|X_i'\beta)$ is linear in $X_i'\beta$.}

As with Ichimura's model, optimization of the maximum likelihood function might become difficult. Apart from computational difficulties, optimization could lead to multiple local minima because the objective function might not be unimode or convex. \footnote{Zhou and Lang (1995) provide an alternative using an ``easy to compute'' semiparametric estimator.}
% section section_about_references_within_the_document (end)

\section{Bandwidth Selection} % (fold)
\label{sec:Bandwidth Selection}

Ichimura (1993) suggests that the smoothing parameter $h$ should be chosen so as to satisfy
 $ln(h)/[nh^{3 + \frac{3}{v-1}}] \rightarrow 0$ and $nh^8 \rightarrow 0$ as $n \rightarrow \infty $, where $v \geq 3$ is a posititive integer whose specific values depend on the existence of a number of finite moments  of $Y$ along with the smoothness of the unknown function $g(\cdot)$. The range of permissible smoothing parameters allows for optimal smoothing. That is, $h_n = O(n^{-\frac{1}{5}})$.
Alternatively, H{\"a}rdle et al. (1993) suggest selecting $h_n$ and $\beta$ simultaneously by minimizing

\begin{equation}
M(\beta, h_n) = \sum_i \left[ Y_i - \hat{G}_{-i}(X_i'\beta, h_n) \right]^2\mathbf{1}{(X_i \in A_\delta)},
\end{equation}

where $\hat{G}_{-i}(X_i'\beta, h_n) = \hat{G}_{-i}(X_i'\beta)$ and $A_\delta$ is the trimming set.	
\newpage 

\bibliographystyle{natdin}
	\bibliography{references} % expects file "references.bib"
	\addcontentsline{toc}{section}{References}
\end{document}