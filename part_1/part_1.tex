\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
% package for including graphics with figure-environment
\usepackage{graphicx}
\usepackage{hyperref}
% colors for hyperlinks
% colored borders (false) colored text (true)
\hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

% package for bibliography
\usepackage[authoryear,round]{natbib}
% package for expectation signs
\usepackage{amsmath,amssymb,mathtools,bm,etoolbox}
%\documentclass[a4paper,11pt]{report} 
<<<<<<< HEAD
=======
\usepackage{breqn}
\usepackage{amsmath}
>>>>>>> isammarques
\usepackage{enumitem} 
\usepackage{amsmath, amsthm, amssymb}
\usepackage{amsmath}
\newcommand{\abs}[1]{ \left\lvert#1\right\rvert} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% package for header
\usepackage[automark]{scrpage2}
\pagestyle{scrheadings}
\ihead[]{Name of students}
\ohead[]{\today}
\cfoot[]{\pagemark} 
\setheadsepline[122mm]{0.3mm}
<<<<<<< HEAD
=======

>>>>>>> isammarques
\begin{document}
	\title{
	%\begin{figure}[!ht]
	%	\flushleft
	%		\includegraphics[width=0.7\textwidth]{logo.eps}
	%\end{figure}
	\vspace{1cm}
	\Huge Here you can insert the title of \\ your seminar paper \\
	}
	
	\vspace{1cm}
	
	% if you are the only author, you might use the following
	% \author{Name of student}	
	
	% Insert here your name and correct mail address
	\author{\Large \href{mailto:first.student@smail.fh-koeln.de}{First Student} \and \Large \href{mailto:second.student@smail.fh-koeln.de}{Second Student}
	\vspace{1cm}}
	
	% name of the course and module
	\date{
	\large Module: Modulename \\ Course: Coursename \\ 
	\vspace{0.8cm}
	\large Lecturer: Name of the lecturer \\
	\vspace{1cm}
	\today
	}

	\maketitle
	\setlength{\parindent}{0pt}

\vspace{2cm}
\begin{abstract}
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

\end{abstract}
	\newpage
	\tableofcontents
	\newpage
	
\section{Introduction} % (fold)
\label{sec:introduction}

Semiparametric single index models are widely used in applied econometrics and applied to a variety of settings.
In this section, we elaborate on the model's main features and contributions to the class of semiparametric models. Essentially, while the risk of mispecification is reduced relative to parametric models, it also avoids inconveniences of fully nonparametric models such as the curse of dimensionality, difficulty of interpretation, and the lack of extrapolation capability.
<<<<<<< HEAD
  
\vspace{5mm} 

Following Li and Racine (2007) notation, the model 

\begin{eqnarray}
Y = g(X'\beta_0) + \epsilon,  % with i =1,2 ..., n?, iid?
=======
 
\vspace{2mm} 
Following Li and Racine (2007) notation, the model
 
\begin{eqnarray}
Y = g(X'\beta) + \epsilon,  % with i =1,2 ..., n?, iid?
>>>>>>> isammarques
\end{eqnarray}

where

\begin{enumerate}
<<<<<<< HEAD
	\item Y is the dependent variable, $X\in \mathbb{R}^{q}$ is a vector of explanatory variables, $\beta_0$ is the q $\times$ 1 vector of unknown parameters; % Y in interval from 0 to 1?
	\item $X'\beta_0$ is a single index because it is a scalar;
=======
	\item (X,Y) is an i.i.d. sample; %Xi??
	\item Y is the dependent variable, $X\in \mathbb{R}^{q}$ is a vector of explanatory variables, $\beta$ is the q $\times$ 1 vector of unknown parameters; % Y in interval from 0 to 1?
	\item $X'\beta$ is a single index because it is a scalar;
>>>>>>> isammarques
	\item $ E(\epsilon|x) = 0 $;
	\item $g: \mathbb{R} \rightarrow \mathbb{R} $ is not known; % dimensions?
\end{enumerate}

is a single index model.
<<<<<<< HEAD

\vspace{5mm} 
=======
\vspace{2mm}
>>>>>>> isammarques

The above model is referred to as a linear single index model by Ichimura (1993). This model is semiparametric as the functional form of the linear index is specified, while $g(\cdot)$ is left unspecified and the conditional probability of $\epsilon$ conditioned on X is not specified except $ E(\epsilon|X) = 0 $. 

While Y can be of either discrete or continuous character, for illustrative purposes, we now analyze binary choice models in the semiparametric single index setting proposed by Li and Racine (2007). More specifically, by accepting the parametric linear index assumption governing choices while not specifying the unknown distribution of the error term, we obtain a semiparametric single index model. Thus, when considering the relationship between a binary dependent variable (Y) and covariates (X) this relationship can be modelled as follows:

\begin{eqnarray}
    Yi = 
    \begin{cases}
      1, & \text{if}\ Y_i^* \stackrel{def}{=} \alpha + X_i'\beta + u_i > 0 \\
      0, & \text{if}\ Y_i^* = \alpha + X_i'\beta + u_i \leq 0
    \end{cases}
\end{eqnarray}

where $Y^*$ is a latent variable.
Consider Y representing labor force participation, equal to 1 if the individual participates in the labor market and 0 otherwise. The explanatory variables can contain a series of economic factors that might influence labor participation, such as gender, age, marital status and education. Assuming a linear relationship between labor force participation and potential determinants, the empirical analysis focuses on the estimation of $\beta$.
Parametric methods to estimate $\beta$ require assumptions on the distribution of the error term $u$. A common assumption in the parametric framework is $ u \sim N(0, \sigma^2)$. With further identification conditions such as $\sigma = 1$, $\beta_$ can be jointly identified (see Madalla (1986)) and we can use maximum likelihood to estimate it.  Let $F_u(\cdot)$ denote the true CDF of $u$. Nevertheless, excluding the assumption of normality of the error term would in general lead to inconsistent estimates. Then we get:


\[ 
\begin{split}
<<<<<<< HEAD
E(Y|x) & = \sum_{y=0,1} yP(y|x) \\
 & = 1 \times P(Y=1|x) + 0 \times P(Y=0|x) \\
 & = P(Y=1|x) \\ 
 & = P(\alpha + x'\beta + u_i > 0) \\
 & = P(u_i > -(\alpha + x'\beta)) \\
 & = 1 - P(u_i \leq -(\alpha + x'\beta)) \\
=======
E(Y|x) & = \sum_{y=0,1} yP(y|x) = 1 \times P(Y=1|x) + 0 \times P(Y=0|x) \\
 & = P(Y=1|x) = P(\alpha + x'\beta + u_i > 0) \\
 & = P(u_i > -(\alpha + x'\beta)) = 1 - P(u_i \leq -(\alpha + x'\beta)) \\
>>>>>>> isammarques
 & = 1 - F(-(\alpha + x'\beta))
\end{split}
\]

where $F(\cdot)$ is the cummulative distribution function (CDF) of $u$. If $ u \sim N(0,1)$ is true, then $u$ has a symmetric distribution and $1 - F(-(\alpha + x'\beta)) = F(\alpha + x'\beta)$. Then model (2) is commonly referred to as a Probit:

\begin{equation}
E(Y|X) = P(Y=1|x) = \Phi(\alpha + x'\beta),
\end{equation}

where $\Phi$ is the CDF of a standard normal variable. On the other hand, for $u$ having a symmetric logistic distribution, we obtain:

\begin{equation}
E(Y|X) = P(Y=1|x) = \frac{1}{1 + e^{\alpha + x'\beta}},
\end{equation}

From (3) and (4) we can see that different functional forms for $u$ lead to different functional forms for the conditional probability of $Y = 1$. Therefore, consistent parametric estimation of $E(Y|X) = P(Y=1|x)$ requires the \textit{correct} distribution specification of $u$. 

<<<<<<< HEAD
Generally speaking, model (1) contains many widely used parametric models that assume that $g$ is known up to a finite-dimensional parameter. If $g$ is the identity function we obtain a linear model. If $g$ is the cumulative normal or logistic distribution, we obtain a binary probit or logit model. Further, assuming $g(X'\beta_0) = E(Y|X=x)$ model (1) represents a tobit model:


\[
    Y = max (0, g(X'\beta_0) + \epsilon),
\]

where $\epsilon$ is an unobserved, normally distributed random variable that has mean zero and is independent of X. 

Thereupon, model (1) with unknown $g(\cdot)$ leads to a more flexible version of parametric models while retaining many of its desirable features. As shown above, model (1) avoids the problem of error distribution mispecification. Further, the single index model achieves dimension reduction and avoids the curse of dimensionality: $g$ is estimated with the same convergence rate in probability as in the case when $X'\beta_0$ is observable and $\beta_0$ achieves the same convergence rate $n^{-\frac{1}{2}}$ as parametric models. Thus, the single index model achieves greater estimation precision than fully nonparametric estimation with multidimensional X and the same precision as a parametric model. Moreover, the assumptions of a single index model are weaker than those of a parametric model and stronger than those of a fully nonparametric model.  \footnote{However, the necessary assumptions for consistent parametric estimation can be relaxed. In particular, the single index model might have weaker assumptions than a fully parameterized model for structural economic models.} Single index models are often easy to compute and interpret. % and as accurate as a one-dimensional nonparametric mean regression for $g$.
=======
Generally speaking, model (1) contains many widely used parametric models that assume that $g$ is known up to a finite-dimensional parameter. Apart from the probit and logic model, if $g$ is the identity function we obtain a linear model. If $g(X'\beta) = E(Y|X=x)$ model (1) represents a tobit model with $Y = max (0, g(X'\beta) + \epsilon)$, where $\epsilon$ is an unobserved, normally distributed random variable that has mean zero and is independent of X. 

%Further, assuming $g(X'\beta) = E(Y|X=x)$ model (1) represents a tobit model:


% \[Y = max (0, g(X'\beta) + \epsilon),\]

%where $\epsilon$ is an unobserved, normally distributed random variable that has mean zero and is independent of X. 

Hence, model (1) with unknown $g(\cdot)$ leads to a more flexible version of parametric models while retaining many of its desirable features. As shown above, model (1) avoids the problem of error distribution mispecification. Further, the single index model avoids the curse of dimensionality as there is only one nonparametric dimension: $\beta$ achieves the same convergence rate $n^{-\frac{1}{2}}$ as parametric models. Thus, the single index model achieves greater estimation precision than fully nonparametric estimation with multidimensional X and the same precision as a parametric model. Moreover, the assumptions of a single index model are weaker than those of a parametric model and stronger than those of a fully nonparametric model.  \footnote{However, the necessary assumptions for consistent parametric estimation can be relaxed. In particular, the single index model might have weaker assumptions than a fully parameterized model for structural economic models.} % and as accurate as a one-dimensional nonparametric mean regression for $g$.
>>>>>>> isammarques


% Identification Racine p 251, why \alpha is not included p.251 racine
% Asymptotically normal!!
% section introduction (end)

\section{Identification} % (fold)
\label{sec:Identification}

<<<<<<< HEAD
Model (1) implies $E(Y|x) = g(x'\beta)$. Thus Y depends on $x$ only through the linear combination $x'\beta_0$, and this relationship is characterized by the link function $g(\cdot)$. Before estimating  $\beta_0$ and $ g(\cdot)$, restrictions must be imposed to ensure identification of the semiparametric model:

\[E(Y|x) = g(x'\beta_0) \]

\vspace{5mm} 
=======
Model (1) implies $E(Y|x) = g(x'\beta)$. Thus Y depends on $x$ only through the linear combination $x'\beta$, and this relationship is characterized by the link function $g(\cdot)$. Before estimating  $\beta$ and $ g(\cdot)$, restrictions must be imposed to ensure identification of the semiparametric model:

\[E(Y|x) = g(x'\beta) \]

\vspace{2mm} 
>>>>>>> isammarques

For the discussion of the identification strategy we follow Li and Racine (2007).


\newtheorem{prop}{Proposition}

<<<<<<< HEAD
\begin{prop}
(Identification of a Single Index Model). 
Identification of $\beta_0$ and $g(\cdot)$ in model (1) requires:
\begin{enumerate}[label=(\roman*)]
\item x should not contain a constant and it must contain at least one continuous variable. Furthermore, $\norm{ \beta_0 } = 1$.
\item $g(\cdot)$ is differentiable and it is not a constant function on the support of $x'\beta_0$.
\item For the discrete components of $x$, varying the values of the discrete variables will not divide the support of $x'\beta_0$ into disjoint subsets.
=======
\begin{prop}[Identification of a Single Index Model] 
Identification of $\beta$ and $g(\cdot)$ in model (1) requires:
\begin{enumerate}[label=(\roman*)]
\item x should not contain a constant and it must contain at least one continuous variable with nonzero coefficient. Furthermore, $\norm{ \beta} = 1$. % or one component of \beta =1
\item $g(\cdot)$ is differentiable and it is not a constant function on the support of $x'\beta$.
\item For the discrete components of $x$, varying the values of the discrete variables will not divide the support of $x'\beta$ into disjoint subsets.
>>>>>>> isammarques
\end{enumerate}
\end{prop}

% Proofs?

<<<<<<< HEAD
\textit{Intuition}. We start with \textit(i) and by emphasizing that the elements of $x$ cannot suffer from multicollinearity, i.e., $Pr(x'\alpha = c)=1$ where $\alpha$ is a constant and $c$ is a scalar. Further, the requirement that $x$ contains at least one continuous variable prevents that $x$ as well as the scalar variable $ v = x'\beta_0$ for any vector $\beta_0$ have a finite support. Then $E(Y|X = x) = g(x'\beta)$ would impose only a finite number of restrictions  on $g(\cdot)$, leading to a infinite number of different choices for $g(\cdot)$ and $\beta_0$ that satisfy those restrictions. \fotenote{Note however that if $g(\cdot)$ is assumed to be increasing, we can identify bounds on the components of $\beta_0$. See Horowitz (2009) for more concrete examples.} In addition, \textit{location normalization} and \textit{scale normalization} requirements are necessary. Define the function $g^*$ by the relation $g^*(\gamma + v\delta) = g(v)$ for all $v$ in the support of $x´\beta_0$. Then

\begin{equation}
E(Y|X = x) = g(x'\beta_0)
=======
\textit{Intuition}. We start with \textit(i) and by emphasizing that the elements of $x$ cannot suffer from multicollinearity, i.e., $Pr(x'\alpha = c)=1$ where $\alpha$ is a constant and $c$ is a scalar. Further, the requirement that $x$ contains at least one continuous variable with nonzero coefficient prevents that $x$ as well as the scalar variable $ v = x'\beta$ for any vector $\beta$ have a finite support. Then $E(Y|X = x) = g(x'\beta)$ would impose only a finite number of restrictions  on $g(\cdot)$, leading to a infinite number of different choices for $g(\cdot)$ and $\beta$ that satisfy those restrictions. \fotenote{Note however that if $g(\cdot)$ is assumed to be increasing, we can identify bounds on the components of $\beta$. See Horowitz (1998) for more concrete examples.} In addition, \textit{location normalization} and \textit{scale normalization} requirements are necessary. Define the function $g^*$ by the relation $g^*(\gamma + v\delta) = g(v)$ for all $v$ in the support of $x´\beta$. Then

\begin{equation}
E(Y|X = x) = g(x'\beta)
>>>>>>> isammarques
\end{equation}

and

\begin{equation}
<<<<<<< HEAD
E(Y|X = x) = g^*(\gamma + x'\beta_0\delta)
\end{equation}

The models (5) and (6) are observationally equivalent. Thus, $\beta_0$ and $g$ are not identified unless restrictions are imposed to uniquely specify $\gamma$ and $\delta$. The restriction on $\gamma$ is then a \textit{location normalization} and it can be satisfied by for example requiring $x$ not to include a constant. The restriction on $\delta$ is a \textit{scale normalization}. In this case we use the approach that assumes that the vector $\beta_0$ has unit length, i.e., $\norm{ \beta_0 } = 1$ using an Euclidean norm.
As to what concerns part (ii), $g(\cdot)$ cannot be a constant function, as otherwise $\beta_0$ is not identified. What makes the identification of $E(Y|X = x)$ possible is that it remains constant if $x$ changes in a way such that $x'\beta_0$ stays constant. However $P(x'\beta_0 = c)=0$, for $x'\beta_0$ continuously distributed and for some constant $c$. For $g$ differentiable, the set of $x$ for which $x'\beta_0$ is within any specified nonzero distance of $c$ has nonzero probability for $c$ in the interior of the support of $x'\beta_0$ and we identify $\beta_0$ by the approximate constancy of $x'\beta_0$.
The violation of (iii) might for example lead to the violation of \textit{location normalization} and \textit{scale normalization}. For a concrete example see Horowitz (2009).

\section{Section about quotations} % (fold)
\label{sec:section_about_quotations}


\section{Section about references within the document} % (fold)
\label{sec:section_about_references_within_the_document}


=======
E(Y|X = x) = g^*(\gamma + x'\beta\delta)
\end{equation}

The models (5) and (6) are observationally equivalent. Thus, $\beta$ and $g$ are not identified unless restrictions are imposed to uniquely specify $\gamma$ and $\delta$. The restriction on $\gamma$ is then a \textit{location normalization} and it can be satisfied by for example requiring $x$ not to include a constant. The restriction on $\delta$ is a \textit{scale normalization}. In this case we use the approach that assumes that the vector $\beta$ has unit length, i.e., $\norm{ \beta } = 1$ using an Euclidean norm. % or one component of \beta =1 in Bruce
As to what concerns part (ii), $g(\cdot)$ cannot be a constant function, as otherwise $\beta$ is not identified. What makes the identification of $E(Y|X = x)$ possible is that it remains constant if $x$ changes in a way such that $x'\beta$ stays constant. However $P(x'\beta = c)=0$, for $x'\beta$ continuously distributed and for some constant $c$. For $g$ differentiable, the set of $x$ for which $x'\beta$ is within any specified nonzero distance of $c$ has nonzero probability for $c$ in the interior of the support of $x'\beta$ and we identify $\beta$ by the approximate constancy of $x'\beta$.
To understand why condition (iii) it is necessary consider a Single Index Model in which X as a continuous component $X_1$ with support $\big[0,1\big]$, and one discrete component $X_2$ whose support is $\{0,1\}$. Assume $X_1$ and $X_2$ are independent, g is strictly increasing and non periodic and $\beta_1$ as a \textit{scale normalization}. Then $E(Y|(x_1,0)) = g(x_1)$ and $E(Y|(x_1,1)) = g(x_1 + \beta_2)$. If $\beta_2 > 1$ the support of $ X_1 + \beta_2$ is disjoint from $\big[0,1\big]$ and $\beta_2$ becomes an intercept in the model $E(Y|(x_1,1))$. As an intercept term is not identified in the model, $\beta_2$ is not identified. However for $beta_2 >1$ the support of $X_1$ and $X_1 + \beta_2$. The $\beta_2$ can be identified by solving

\begin{equation}
E[Y| X = (x_1,0)] = g( x_1 + \beta_2),
\end{equation}

for $x_1 \in \big[\beta_2,1\big]$. \footnote{Note that if g was periodic on $\big[\beta_2,1\big]$, (7) would have at least two solutions and $\beta_2$ would not be identified.}


\section{Ichimura's estimation model} % (fold)
\label{sec:Ichimura's estimation model}

In this section we analyze Ichimura's (1993) estimation method for semiparametric models. Were $g(\cdot)$ to be known and a nonlinear least squares method could be used to estimate $\beta_0$ by minimizing:

\begin{equation}
\sum_{i = 1}\big[Y_i - g(X_i'\beta)\big]^2
\end{equation}

with respect to $\beta$.
In this case as $g(\cdot)$ is not know we need to estimate it. However, this cannot be done directly with kernel estimation as $\beta_0$ is also unknown. Still, for a given $\beta$ we can estimate:

\begin{equation}
G(X_i'\beta) \stackrel{def}{=} E(Yi|X_i'\beta) = E[g(X_i'\beta_0)|X_i'\beta]
\end{equation}
 by the kernel method. Ichimura (1993) proposes modifications of the usual kernel estimation to estimate $G(X_i'\beta)$. First, observation $i$ is excluded from the calculation of $G(X_i'\beta)$. Second, the denominator contain a term $\hat{p}_{-i}(X_i'\beta)$. This is a leave-one-out Nadaraya-Watson (NW) kernel estimator and is given by:

\begin{equation}
\hat{G}_{-i}(X_i'\beta) \equiv \hat{E}_{-i}(Yi|X_i'\beta) = \frac{(nh)^{-1}\sum_{j=1, j &\neqi }^{n}  Y_jK(\frac{X_j'\beta - X_i'\beta}{h})}{\hat{p}_{-i}(X_i'\beta)},
\end{equation}

where $\hat{p}_{-i}(X_i'\beta) = (nh)^{-1}\sum_{j=1,j &\neq i}^{n}  Y_jK(\frac{X_j'\beta - X_i'\beta}{h})}$.  The denominator $\hat{p}(X_i'\beta)$ is random and it becomes necessary to trim out small values. Let $p(x'\beta)$ denote the probability density function (PDF) of $X_i'\beta$ and $A_\delta$ and $A_n$ be the following sets:

\[ A_\delta = \{ x : p(x'\beta) \geq \delta, \text{ for all }  \beta \in \mathcal{B} \}
\]

where $\delta > 0$ is a constant, $\mathcal{B} \in \mathbb{R}^q$, and

\[ A_n = \{ x : \norm{x - x^*} \geq 2h \text{ for some } x^* \in A_\delta \}
\]

Then for $x \in A_\delta$ the denominator does not get too close to zero. The set $A_n$ where $\norm{\cdot}$ is a Euclidean norm, is larger than $A_\delta$ but as $ n \rightarrow \infty $, $h \rightarrow 0$, $A_n$ shrinks to $A_\delta$. With this in mind it is possible to choose $\beta$ by minimizing the objective function:

\begin{equation}
S_n(\beta) = \sum_{i=1}^{n}  [Y_i - \hat{G}_{-i}(X_i'\beta)]^2w(x_i)\mathbf{1}{(X_i \in A_n)}
\end{equation}

where $\mathbf{1}{(Xi \in A_n)$ is a trimming function that ensures that the random denominator is positive with high probability, and $w(xi)$ is a nonnegative weighting function.

According to Li and Racine (2007), to derive the assymptotic distribution of $\hat{\beta}$ we need the following assumptions:

\newtheorem{theorem}{Theorem} 

\begin{theorem}
According to Ichimura (1993), under the assumptions:

\begin{enumerate}
	\item The set $A_\delta$ is compact, and the weight function $w(\cdot)$ is bounded and positive on $A_\delta$. Define the set $D_z = \{ z : z = x'\beta. \beta \in \mathcal{B}, x \in A_\delta\}$. Letting $p(\cdot)$ denote the PDF of $z \in D_z$, $p(\cdot)$ is bounded below by a positive constant for all $z \in D_z$.
	\item $g(\cdot)$ and $p(\cdot)$ are three times differentiable with respect to $z = x´\beta$. The third derivatives are Lipschitz continuous uniformly over \mathcal{B} for all $z \in D_z$.
	\item The kernel function is a bounded second order kernel having bounded support, is twice differentiable, and its second derivative is Lipschitz continuous.
	\item $E|Y^m| < \infty$ for some $m \geq 3$. cov(Y|x) is bounded and bounded away from zero for all $x \in A_\delta$. $ln(h)/[nh^{3 + \frac{3}{m-1}}] \rightarrow 0$ and $nh^8 \rightarrow 0$ as $n \rightarrow \infty $ .
	
\end{enumerate}

we have

\[ \sqrt{n}(\hat{\beta} - \beta_0}) \rightarrow N(0,\Omega_I) \] in ditribution, where $\Omega_I = V^{-1}\Sigma V^{-1}$, and

\[\Sigma = E\{w(X_i)\sigma^2(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0)) \times (X_i - E_A(X_i|X_i'\beta_0))'\},\]

with $g_i^{(1)} = [\partial g(v)/\partial v]|_v = X_i'\beta_0, E_A(X_i|v) = E(X_i|x_A'\beta_0 = v)$ with $x_A$ having the distribution of X, conditional on $Xi \in A_\delta$, and

\[ V = E[w(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0))(X_i - E_A(X_i|X_i'\beta_0))'].\]

\end{theorem}


A consistent estimator of $\Omega_I$ is given by

\[ \hat{\Omega}_I = \hat{V}^{-1}\hat{\Sigma}\hat{V}^{-1}, \]

where $\hat{V} = n^{-1}\sum_{i} w(X_i)\hat{g}^{(1)}(X_i'\hat{\beta})(X_i - \hat{E}(X_i|X_i'\beta))(X_i - \hat{E}(X_i|X_i'\beta))'$, $\hat{\Sigma} = n^{-1}\sum_{i} w(X_i)\hat{u}_{i}^{2}\hat{g}^{(1)}(X_i'\hat{\beta})(X_i - \hat{E}(X_i|X_i'\beta))'$, $\hat{u}_i = Y_i - \hat{g}(X_i'\beta)$, $\hat{g}^{(1)}(X_i'\beta) = [\partial \hat{g}_{-i}/\partial \beta]|_{\beta=\hat{\beta}$, $\hat{g}_(-i)(X_i'\beta)$ is defined in (10), $\hat{E}(X_i|X_i'\beta)' = \sum_{j} X_jK((X_i - X_j')'\hat{\beta})/ \sum_{j}K((X_i - X_j)'\hat{\beta}).$

For what follows ignore the trimming set $A_\delta$, $w(\cdot)$ and assume that the minimization over $\beta$ is done over the shrinking set $\mathcal{B}_n = \{ \beta : \norm{\beta - \beta_0} \geq Cn^{-1/2}$ for some constant $ C > 0$. This follows H\"ardle, Hall and Ichimura (1993). Further, according to Ichimura (1993) $\beta - \beta_0 = O(n^{-\frac{1}{2}})$. With this in mind and $ h \in \{h : C_1n^{-\frac{1}{5}} \geq h \geq C_2n^{-\frac{1}{5}}\}$ for some $C_2 > C_1 > 0$ we can establish a proof for Theorem 1. First,

\begin{equation}
S_n(\beta) = n^{-1} \sum_i[Y_i - \hat{G}_{-i}(X_i'\beta)]^2 = S_n(\beta) + T_n + o_p(1),
\end{equation}  

where $S(\beta) = n^{-1} \sum_i [Y_i - G(X_i'\beta)]^2$, and

\[ T_n = \sum_{i=1}^{n}[\hat{G}_{-i}(X_i'\beta_0) - g(X_i'\beta_0)]^2 \]

is a term that is independent of $\beta$. It can be shown that $S(\beta) = O(1)$. Thus minimizing $S_n(\beta)$ with respect to $\beta$ is asymptotically equivalent to minimizing $S(\beta)$. It can also be shown that $\hat{E}[Y_i|X_i'\beta]$ can be replaced with $E[g(X_i'\beta)|X_i'\beta]$. Then,


\begin{align*}
S_{n}(\beta) & =\frac{1}{n}\sum_{i}\{Y_i - \hat{G}_{-i}(X_i'\beta)\}^2 = \frac{1}{n}\sum_{i} \{g(X_i'\beta_0) + \epsilon_i - E[g(X_i'\beta_0)|X_i'\beta)]\}^2 \\
		     & = \frac{1}{n}\sum_{i}\{g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta)]\}^2 \\
		     & + 2\frac{1}{n}\sum_i\epsilon_i\{ X_i'\beta_0 - E[g(X_i'\beta_0)|X_i'\beta)]\} + \{ terms\: independent\:of\:\beta \} \\
		     & + o_p(1)
\end{align*}

Using a Taylor expansion at $\beta = \beta_0$ and with Ichimura's (1993) result that $\beta_0 - \beta = O(n^{-\frac{1}{2}})$:

\begin{align*}
g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta)] & = g(X_i'\beta_0) - g(X_i'\beta) \\
											 & - g^{(1)}(X_i'\beta)E[(\beta_0 - \beta)X_i'|X_i'\beta] + O(n^{-1}) \\
				                              & = g^{(1)}(X_i'\beta)( X_i - E[X_i'|X_i'\beta])(\beta_0 - \beta) + O(n^{-1})
\end{align*}

Hence, for $g_{i}^{(1)} = g^{(1)}(X_i'\beta)$ and $v_i = X_i - E[X_i'|X_i'\beta]$:

\begin{align*}
S_{n}(\beta) & = (\beta_0 - \beta)'[\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_iv_i'](\beta_0 - \beta)+ 2\frac{1}{n}\sum_i\epsilon_ig_{i}^{(1)}v_i'(\beta_0 - \beta) \\
			 & + \{terms\:independent\:of \:\beta\} + o_p(n^{-1}) 
\end{align*}

Minimizing in order to $\beta$ and ignoring terms independent of $\beta$ and $o_p(n^{-1})$:

\begin{align*}
\sqrt{n}(\beta_0 - \beta) & = (\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_iv_i')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i}^{(1)}v_i \\
     					  & = (\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_{i0}v_{i0}')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i0}^{(1)}v_{i0} + o_p(1)
\end{align*}

For $g_{i0}^{(1)} = g^{(1)}(X_i'\beta_0)$ and $v_{i0} = X_i - E[X_i'|X_i'\beta_0]$. With this in mind, the Central Limit Theorem (CLT) and Law of Large Numbers (LLN) we obtain the result from Theorem 1 if we replace $w(X_i) = 1$ there.










%\begin{equation}
%W_0(\bar{\beta} - \beta_0) = V_0 + (s.o.),
%\end{equation}


%\[  W_0 = \sum_{i=1}^{n} [g^{(1)}(X_i'\beta_0)]^2[X_i - E(X_i|X_i'\bet_0][X_i - E(X_i|X_i'\beta_0)]' \]

%and

%\[ V_0 = \sum_{i=1}^{n} u_ig^{(1)}(X_i'\beta_0)[X_i - E(X_i|X_i'\beta_0).\]

%Then, by the standard law of large numbers and a central limit theorem (CLT) argument, we have

%\begin{equation}
%\sqrt{n}(\bar{\beta} - \beta_0) = (W_0/n)^{-1}n^{-\frac{1}{2}}V_0 + o_p(1) \overset{d}{\to} N(0,\Sigma_0),
%\end{equation}

%where $\Sigma_0$ is the same as $\Sigma_I$, except that for the former $W(X_i) = 1$.



\section{Section about references within the document} % (fold)
\label{sec:section_about_references_within_the_document}
>>>>>>> isammarques


% section section_about_references_within_the_document (end)

\subsection{Subsection within Foundations} % (fold)
\label{sub:subsection_within_foundations}
<<<<<<< HEAD
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
=======

>>>>>>> isammarques
% subsection subsection_within_foundations (end)

\subsection{Another subsection within Foundations} % (fold)
\label{sub:another_subsection_within_foundations}
<<<<<<< HEAD
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
=======

>>>>>>> isammarques
% subsection another_subsection_within_foundations (end)


% section foundations (end)

\section{Methodology} % (fold)
\label{sec:methodology}
<<<<<<< HEAD
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua \citep{Dix04}. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
=======

>>>>>>> isammarques
% section methodology (end)

\section{Results} % (fold)
\label{sec:results}
<<<<<<< HEAD
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua \citep[p. 48]{Baddeley:1974ts}. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
=======

>>>>>>> isammarques
% section results (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}
<<<<<<< HEAD
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
=======

>>>>>>> isammarques
% section conclusion (end)


\newpage 

\bibliographystyle{natdin}
	\bibliography{references} % expects file "references.bib"
	\addcontentsline{toc}{section}{References}
\end{document}