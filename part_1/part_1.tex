\documentclass[a4paper]{article}
\setlength{\headheight}{1.1\baselineskip}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
% package for including graphics with figure-environment
\usepackage{graphicx}
\usepackage{hyperref}
% colors for hyperlinks
% colored borders (false) colored text (true)
\hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

% package for bibliography
\usepackage[authoryear,round]{natbib}
% package for expectation signs
\usepackage{amsmath,amssymb,mathtools,bm,etoolbox}
%\documentclass[a4paper,11pt]{report} 
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{enumitem} 
\usepackage{amsmath, amsthm, amssymb}
\usepackage{amsmath}
\newcommand{\abs}[1]{ \left\lvert#1\right\rvert} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% package for header
\usepackage[automark]{scrpage2}
\pagestyle{scrheadings}
\ihead[]{Name of students}
\ohead[]{\today}
\cfoot[]{\pagemark} 
\setheadsepline[122mm]{0.3mm}

\begin{document}
	\title{
	%\begin{figure}[!ht]
	%	\flushleft
	%		\includegraphics[width=0.7\textwidth]{logo.eps}
	%\end{figure}
	\vspace{1cm}
	\Huge Here you can insert the title of \\ your seminar paper \\
	}
	
	\vspace{1cm}
	
	% if you are the only author, you might use the following
	% \author{Name of student}	
	
	% Insert here your name and correct mail address
	\author{\Large \href{mailto:first.student@smail.fh-koeln.de}{First Student} \and \Large \href{mailto:second.student@smail.fh-koeln.de}{Second Student}
	\vspace{1cm}}
	
	% name of the course and module
	\date{
	\large Module: Modulename \\ Course: Coursename \\ 
	\vspace{0.8cm}
	\large Lecturer: Name of the lecturer \\
	\vspace{1cm}
	\today
	}

	\maketitle
	\setlength{\parindent}{0pt}

\vspace{2cm}
\begin{abstract}
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

\end{abstract}
	\newpage
	\tableofcontents
	\newpage
	
\section{Introduction} % (fold)
\label{sec:introduction}

Semiparametric single index models are widely used in applied econometrics and applied to a variety of settings.
In this section, we elaborate on the model's main features and contributions to the class of semiparametric models.  
Essentially, while the risk of mispecification is reduced relative to the overly restrictive but interpretable parametric models, it also avoids inconveniences of fully nonparametric models such as the curse of dimensionality, difficulty of interpretation, and the lack of extrapolation capability.

\vspace{2mm} 
Following Li and Racine (2007) notation, the model
 
\begin{eqnarray}
Y = g(X'\beta) + \epsilon,  % with i =1,2 ..., n?, iid?
\end{eqnarray}

where

\begin{enumerate}
		\item Observations $\{x_i,y_i\}$ are i.i.d.;
		\item Y is the dependent variable, $X\in \mathbb{R}^{q}$ is a vector of explanatory variables, $\beta$ is the q $\times$ 1 vector of unknown parameters; % Y in interval from 0 to 1?
	\item $X'\beta$ is a single index because it is a scalar;
	\item $ E(\epsilon|x) = 0 $;
	\item $g: \mathbb{R} \rightarrow \mathbb{R} $ is not known; % dimensions?
\end{enumerate}

is a single index model.
\vspace{2mm}

The above model is referred to as a linear single index model by Ichimura (1993). This model is semiparametric as the functional form of the linear index is specified, while $g(\cdot)$ is left unspecified and the conditional probability of $\epsilon$ conditioned on X is not specified except $ E(\epsilon|X) = 0 $. 

While Y can be of either discrete or continuous character, for illustrative purposes, we now analyze binary choice models in the semiparametric single index setting proposed by Li and Racine (2007). More specifically, by accepting the parametric linear index assumption governing choices while not specifying the unknown distribution of the error term, we obtain a semiparametric single index model. Thus, when considering the relationship between a binary dependent variable (Y) and covariates (X) this relationship can be modelled as follows:

\[
    Yi = 
    \begin{cases}
      1, & \text{if}\ Y_i^* \stackrel{def}{=} \alpha + X_i'\beta + u_i > 0 \\
      0, & \text{if}\ Y_i^* = \alpha + X_i'\beta + u_i \leq 0
    \end{cases}
\]

where $Y^{*}$ is a latent variable.
Consider Y representing labor force participation, equal to 1 if the individual participates in the labor market and 0 otherwise. The explanatory variables can contain a series of economic factors that might influence labor participation, such as gender, age, marital status and education. Assuming a linear relationship between labor force participation and potential determinants, the empirical analysis focuses on the estimation of $\beta$.
Parametric methods to estimate $\beta$ require assumptions on the distribution of the error term \textit{u}. A common assumption in the parametric framework is $ u \sim N(0, \sigma^{2})$. With further identification conditions such as $\sigma = 1$, $\beta$ can be jointly identified (see Madalla (1986)) and we can use maximum likelihood to estimate it.  Let $F_u(\cdot)$ denote the true CDF of \textit{u}. Nevertheless, excluding the assumption of normality of the error term would in general lead to inconsistent estimates. Then we get:


\[ 
\begin{split}
E(Y|x) & = \sum_{y=0,1} yP(y|x) = 1 \times P(Y=1|x) + 0 \times P(Y=0|x) \\
 & = P(Y=1|x) = P(\alpha + x'\beta + u_i > 0) \\
 & = P(u_i > -(\alpha + x'\beta)) = 1 - P(u_i \leq -(\alpha + x'\beta)) \\
 & = 1 - F(-(\alpha + x'\beta))
\end{split}
\]

where $F(\cdot)$ is the cummulative distribution function (CDF) of $u$. If $ u \sim N(0,1)$ is true, then $u$ has a symmetric distribution and $1 - F(-(\alpha + x'\beta)) = F(\alpha + x'\beta)$. Then model (2) is commonly referred to as a Probit:

\[
E(Y|X) = P(Y=1|x) = \Phi(\alpha + x'\beta),
\]

where $\Phi$ is the CDF of a standard normal variable. On the other hand, for $u$ having a symmetric logistic distribution, we obtain:

\[
E(Y|X) = P(Y=1|x) = \frac{1}{1 + e^{\alpha + x'\beta}},
\]

From (3) and (4) we can see that different functional forms for $u$ lead to different functional forms for the conditional probability of $Y = 1$. Therefore, consistent parametric estimation of $E(Y|X) = P(Y=1|x)$ requires the \textit{correct} distribution specification of $u$. Apart from the probit and logic model, if $g$ is the identity function we obtain a linear model. If $g(X'\beta) = E(Y|X=x)$ model (1) represents a tobit model with $Y = max (0, g(X'\beta) + \epsilon)$, where $\epsilon$ is an unobserved, normally distributed random variable with mean zero and independent of X. 
% Generally speaking, model (1) contains many widely used parametric models that assume that $g$ is known up to a finite-dimensional parameter. 

Hence, model (1) with unknown $g(\cdot)$ leads to a more flexible version of parametric models while retaining many of its desirable features. As shown above, model (1) avoids the problem of error distribution mispecification. Further, the model avoids the curse of dimensionality by reducing the p-dimensional predictor to a univariate single-index. Then $\beta$ achieves the same convergence rate $n^{-\frac{1}{2}}$ as parametric models. Thus, the single index model reaches greater estimation precision than fully nonparametric estimation with multidimensional X and the same precision as a parametric model. Moreover, the assumptions of a single index model are weaker than those of a parametric model and stronger than those of a fully nonparametric model.  \footnote{However, the necessary assumptions for consistent parametric estimation can be relaxed. In particular, the single index model might have weaker assumptions than a fully parameterized model for structural economic models.} % and as accurate as a one-dimensional nonparametric mean regression for $g$.

% section introduction (end)

\section{Identification} % (fold)
\label{sec:Identification}

Model (1) implies $E(Y|x) = g(x'\beta_0)$. Thus Y depends on $x$ only through the linear combination $x'\beta_0$, and this relationship is characterized by the link function $g(\cdot)$. Before estimating  $\beta$ and $ g(\cdot)$, restrictions must be imposed to ensure identification of the semiparametric model:

\begin{equation}
E(Y|x) = g(x'\beta_0)
\end{equation}


\vspace{2mm} 

For the discussion of the identification strategy we follow Li and Racine (2007).


\newtheorem{prop}{Proposition}[section]

\begin{prop}[Identification of a Single Index Model] 
Identification of $\beta_0$ and $g(\cdot)$ in model (1) requires:
\begin{enumerate}[label=(\roman*)]
\item x should not contain a constant and it must contain at least one continuous variable with nonzero coefficient. Furthermore, $\norm{ \beta_0} = 1$. % or one component of \beta =1
\item $g(\cdot)$ is differentiable and it is not a constant function on the support of $x'\beta_0$.
\item For the discrete components of $x$, varying the values of the discrete variables will not divide the support of $x'\beta_0$ into disjoint subsets.
\end{enumerate}
\end{prop}

\textit{Intuition}. We start with (i) and by emphasizing that the elements of \textit{x} cannot suffer from multicollinearity, i.e., $Pr(x'\alpha = c) = 1$ where $\alpha$ is a constant and \textit{c }is a scalar. Further, the requirement that \textit{x} contains at least one continuous variable (with nonzero coefficient) prevents that $x$ as well as the scalar variable $ v = x'\beta_0 $ for any vector $\beta_0$ have a finite support. Then $E(Y|X = x) = g(x'\beta_0)$ would impose only a finite number of restrictions  on $g(\cdot)$, leading to a infinite number of different choices for $g(\cdot)$ and $\beta_0$ that satisfy those restrictions.\footnote{Note however that if $g$ is assumed to be increasing, we can identify bounds on the components of $\beta_0$. See Horowitz (1998) for  concrete examples.} In addition, \textit{location normalization} and \textit{scale normalization} requirements are necessary. Define the function $g^{*}$ by the relation $g^{*}(\gamma + v\delta) = g(v)$ for all \textit{v} in the support of $x'\beta_0$. Then

\begin{equation}
E(Y|X = x) = g(x'\beta_0)
\end{equation}

and

\begin{equation}
E(Y|X = x) = g^*(\gamma + x'\beta_0\delta)
\end{equation}

The models (3) and (4) are observationally equivalent. Thus, $\beta_0$ and $g$ are not identified unless restrictions are imposed to uniquely specify $\gamma$ and $\delta$. The restriction on $\gamma$ is then a \textit{location normalization} and it can be satisfied by for example requiring $x$ not to include a constant. The restriction on $\delta$ is a \textit{scale normalization}. In this case we use the approach that assumes that the vector $\beta_0$ has unit length, i.e., $\norm{ \beta_0 } = 1$ using an Euclidean norm. \footnote{This implies that X must have at least 2 dimensions. Otherwise $\beta$ is simply normalized to 1 and a one-dimensional nonparametric model $E(y|x) = g(x)$ with no semiparametric part is obtained instead.}. Alternatively, we could set one component of $\beta$ to 1. % or one component of \beta =1 in Bruce
As to what concerns part (ii), $g(\cdot)$ cannot be a constant function, as otherwise $\beta_0$ is not identified. What makes the identification of $E(Y|X = x)$ possible is that it remains constant if $x$ changes in a way such that $x'\beta_0$ stays constant. However $P(x'\beta_0 = c)=0$, for $x_0'\beta$ continuously distributed and for some constant $c$. For $g$ differentiable, the set of $x$ for which $x'\beta_0$ is within any specified nonzero distance of $c$ has nonzero probability for $c$ in the interior of the support of $x'\beta_0$ and we identify $\beta_0$ by the approximate constancy of $x'\beta_0$.
To understand why condition (iii) it is necessary consider a Single Index Model in which X as a continuous component $X_1$ with support $\big[0,1\big]$, and one discrete component $X_2$ whose support is $\{0,1\}$. Assume $X_1$ and $X_2$ are independent, g is strictly increasing and non periodic and set $\beta_1 = 1$ as a \textit{scale normalization}. Then $E(Y|(x_1,0)) = g(x_1)$ and $E(Y|(x_1,1)) = g(x_1 + \beta_2)$. For the first case where $X_2 = 0$ the function g is identified on $\big[0,1\big]$. However, for $\beta_2 > 1$ the support of $ X_1 + \beta_2$ is disjoint from $\big[0,1\big]$ and $\beta_2$ is an intercept in the model for $E(Y|(x_1,1))$. Thus, it is not possible to identify $\beta_2$. However, for $\beta_2 < 1$ the support of $X_1$ and $X_1 + \beta_2$ overlap. In particular for the subset $\{X: X_1 \in [ \beta_2, 1] \wedge X_2 = 0\}$, $g(x_1 + \beta_2) = g(v)$ for some $v \in [0,1]$. Then, $g(v)$ can be identified for $v \in [\beta_2, 1]$ by observations of $X_1$ for which $X_2 = 0$. Therefore, $\beta_2$ can be identified by solving

\begin{equation}
E[Y| X = (x_1,0)] = g(x_1 + \beta_2),
\end{equation}

for $x_1 \in \big[\beta_2,1\big]$. \footnote{Note that if g was periodic on $\big[\beta_2,1\big]$, (7) would have at least two solutions and $\beta_2$ would not be identified.}


\section{Ichimura's estimation model} % (fold)
\label{sec:Ichimura's estimation model}

In this section Ichimura's (1993) estimation method for semiparametric models is analyzed. This method exhibits $n^{-\frac{1}{2}}$ efficiency and asymptotic normality. We also investigate a weighting matrix that reaches the semiparametric efficiency bound. Let $\beta_0$ denote the true value of $\beta$. Were $g(\cdot)$ to be known and a nonlinear least squares method could be used to estimate $\beta_0$ by minimizing:

\begin{equation}
\sum_{i = 1}\big[Y_i - g(X_i'\beta)\big]^2
\end{equation}

with respect to $\beta$.
In this case as $g(\cdot)$ is not know we need to estimate it. However, this cannot be done directly with kernel estimation as $\beta_0$ is also unknown. Still, for a given $\beta$ we can estimate:

\begin{equation}
G(X_i'\beta) \stackrel{def}{=} E(Yi|X_i'\beta) = E[g(X_i'\beta_0)|X_i'\beta]
\end{equation}
 by the kernel method. Ichimura (1993) proposes modifications of the usual kernel estimation to estimate $G(X_i'\beta)$. First, observation $i$ is excluded from the calculation of $G(X_i'\beta)$. Second, the denominator contain a term $\hat{p}_{-i}(X_i'\beta)$. This is a leave-one-out Nadaraya-Watson (NW) kernel estimator and is given by:

\begin{equation}
\hat{G}_{-i}(X_i'\beta) \equiv \hat{E}_{-i}(Yi|X_i'\beta) = \frac{(nh)^{-1}\sum_{j=1, j \neq i }^{n}  Y_jK(\frac{X_j'\beta - X_i'\beta}{h})}{\hat{p}_{-i}(X_i'\beta)},
\end{equation}

where $\hat{p}_{-i}(X_i'\beta) = \frac{1}{nh}\sum_{j=1,j \neq i}^{n}Y_{j}K(\frac{X_j'\beta - X_i'\beta}{h})$.  The denominator $\hat{p}(X_i'\beta)$ is random and it becomes necessary to trim out small values. Let $p(x'\beta)$ denote the probability density function (PDF) of $X_i'\beta$ and $A_\delta$ and $A_n$ be the following sets:

\[ A_\delta = \{ x : p(x'\beta) \geq \delta, \text{ for all }  \beta \in \mathcal{B} \}
\]

where $\delta > 0$ is a constant, $\mathcal{B} \in \mathbb{R}^q$, and

\[ A_n = \{ x : \norm{x - x^*} \geq 2h \text{ for some } x^* \in A_\delta \}
\]

Then for $x \in A_\delta$ the denominator does not get too close to zero. The set $A_n$ where $\norm{\cdot}$ is a Euclidean norm, is larger than $A_\delta$ but as $ n \rightarrow \infty $, $h \rightarrow 0$, $A_n$ shrinks to $A_\delta$. With this in mind it is possible to choose $\beta$ by minimizing the objective function:

\begin{equation}
S_n(\beta) = \sum_{i=1}^{n}  [Y_i - \hat{G}_{-i}(X_i'\beta)]^2w(x_i)\mathbf{1}{(X_i \in A_n)}
\end{equation}

where $\mathbf{1}{(Xi \in A_n)}$ is a trimming function that ensures that the random denominator is positive with high probability, and $w(xi)$ is an appropriate nonnegative weighting function for possible heteroskedasticity.

According to Li and Racine (2007), to derive the assymptotic distribution of $\hat{\beta}$ we need the following assumptions:

\newtheorem{theorem}{Theorem}[section]

\begin{theorem}
According to Ichimura (1993), under the assumptions:

\begin{enumerate}
	\item The set $A_\delta$ is compact, and the weight function $w(\cdot)$ is bounded and positive on $A_\delta$. Define the set $D_z = \{ z : z = x'\beta, \beta \in \mathcal{B}$,$ x \in A_\delta\}$. Letting $p(\cdot)$ denote the PDF of $z \in D_z$, $p(\cdot)$ is bounded below by a positive constant for all $z \in D_z$.
	\item $g(\cdot)$ and $p(\cdot)$ are three times differentiable with respect to $z = x'\beta$. The third derivatives are Lipschitz continuous uniformly over $\mathcal{B}$ for all $z \in D_z$.
	\item The kernel function is a bounded second order kernel having bounded support, is twice differentiable, and its second derivative is Lipschitz continuous.
	\item $E|Y^m| < \infty$ for some $m \geq 3$. cov(Y|x) is bounded and bounded away from zero for all $x \in A_\delta$. $ln(h)/[nh^{3 + \frac{3}{m-1}}] \rightarrow 0$ and $nh^8 \rightarrow 0$ as $n \rightarrow \infty $ .
	
\end{enumerate}

we have

\[ \sqrt{n}(\hat{\beta} - \beta_0) \rightarrow N(0,\Omega_I) \] in ditribution, where $\Omega_I = V^{-1}\Sigma V^{-1}$, and

\[\Sigma = E\{w(X_i)\sigma^2(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0)) \times (X_i - E_A(X_i|X_i'\beta_0))'\},\]

with $g_i^{(1)} = [\partial g(v)/\partial v]|_{v = X_i'\beta_0}, E_A(X_i|v) = E(X_i|x_A'\beta_0 = v)$ with $x_A$ having the distribution of X, conditional on $Xi \in A_\delta$, and

\[ V = E[w(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0))(X_i - E_A(X_i|X_i'\beta_0))'].\]

\end{theorem}


A consistent estimator of $\Omega_I$ is given by

\[ \hat{\Omega}_I = \hat{V}^{-1}\hat{\Sigma}\hat{V}^{-1}, \]

where $\hat{V} = n^{-1}\sum_{i} w(X_i)\hat{g}^{(1)}(X_i'\hat{\beta})(X_i - \hat{E}(X_i|X_i'\beta))(X_i - \hat{E}(X_i|X_i'\beta))', \hat{\Sigma} = n^{-1}\sum_{i} w(X_i)\hat{\epsilon}_{i}^{2}\hat{g}^{(1)}(X_i'\hat{\beta})(X_i - \hat{E}(X_i|X_i'\beta))', \hat{\epsilon}_i = Y_i - \hat{g}(X_i'\beta), \hat{g}^{(1)}(X_i'\beta) \linebreak
= [\partial \hat{g}_{-i}/\partial \beta]|_{\beta=\hat{\beta}}$, $\hat{g}_{-i}(X_i'\beta)$ is defined in (10), $\hat{E}(X_i|X_i'\beta)' = \sum_{j} X_jK((X_i - X_j')'\hat{\beta})/ \sum_{j}K((X_i - X_j)'\hat{\beta}).$

For what follows ignore the trimming set $A_\delta$ is ignored and $w(\cdot)$ is set to 1. Furthermore, according to Ichimura (1993) $\beta - \beta_0 = O(n^{-\frac{1}{2}})$. 


%and assume that the minimization over $\beta$ is done over the shrinking set $\mathcal{B}_n = \{ \beta : \norm{\beta - \beta_0} \geq Cn^{-1/2}$ for some constant $ C > 0$. This follows H\"ardle, Hall and Ichimura (1993). With this in mind and $ h \in \{h : C_1n^{-\frac{1}{5}} \geq h \geq C_2n^{-\frac{1}{5}}\}$ for some $C_2 > C_1 > 0$ we can establish a proof for Theorem 1. First,

\begin{align*}
S_{n}(\beta) & = \frac{1}{n}\sum_i \{ Y_i - \hat{G}_{-i}(X_i'\beta)\}^2 = \frac{1}{n}\sum_i\{Y_i - \hat{G}_{-i}(X_i'\beta) +  \hat{G}_{-i}(X_i'\beta_0) \\
			 & - \hat{G}_{-i}(X_i'\beta_0) \}^2 = \frac{1}{n} \sum_i \{Y_i - G(X_i'\beta) + o_p(1) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - \hat{G}_{-i}(X_i'\beta_0) \}^2 = \frac{1}{n}\sum_i \{ Y_i - G(X_i'\beta) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - g(X_i'\beta_0) + o_p(1) \}^2 = \frac{1}{n} \sum_i \{ g(X_i'\beta_0) + \epsilon_i - G(X_i'\beta) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - g(X_i'\beta_0) + o_p(1) \}^2 = \frac{1}{n} \sum_i \{ \epsilon_i + \hat{G}_{-i}(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta] \\
			 & + o_p(1) \}^2 = \sum_i \{ g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta] +  \epsilon_i + o_p(1)\}^2 
\end{align*}

Using a Taylor expansion at $\beta = \beta_0$:

\begin{align*}
g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta)] & = g(X_i'\beta_0) - g(X_i'\beta) \\
											 & - g^{(1)}(X_i'\beta)E[(\beta_0 - \beta)X_i'|X_i'\beta] + O(n^{-1}) \\
				                              & = g^{(1)}(X_i'\beta)( X_i - E[X_i'|X_i'\beta])(\beta_0 - \beta) + O_p(n^{-1})
\end{align*}


Note $O_p(n^{-2}) = n^{-2}O_p(1) = o_p(1)$ and thus $O_p(n^{-1}) = o_p(1)$. Also, considering $E(\epsilon) = 0$ and the Chebyshev's Inequality, we have $\sum_i\epsilon_io_p(1) = o_p(1)$. Hence, for $g_{i}^{(1)} = g^{(1)}(X_i'\beta)$ and $v_i = X_i - E[X_i'|X_i'\beta]$:

\begin{align*}
S_{n}(\beta) & = \frac{1}{n}\sum_i (\beta_0 - \beta)'[\frac{1}{n}\sum_i (g_i^{(1)})^2v_iv_i'](\beta_0 - \beta) \\
             & + 2\frac{1}{n}\sum_i\epsilon_ig_i^{(1)}v_i'(\beta_0 - \beta) + \frac{1}{n}\sum_i \epsilon_i^2 + o_p(1)
\end{align*}

Minimizing in order to $\beta$ and ignoring terms independent of $\beta$ and $o_p(1)$:

\begin{align*}
\sqrt{n}(\beta_0 - \beta) & = (\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_iv_i')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i}^{(1)}v_i \\
     					  & = (\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_{i0}v_{i0}')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i0}^{(1)}v_{i0} + o_p(1).
\end{align*}

For $g_{i0}^{(1)} = g^{(1)}(X_i'\beta_0)$ and $v_{i0} = X_i - E[X_i'|X_i'\beta_0]$. With this in mind, the Central Limit Theorem (CLT) and Law of Large Numbers (LLN) we obtain the result from Theorem 1 if we replace $w(X_i) = 1$ there.


%Recall when we described identiOÌˆcation that we required the dimension of Xi to be 2 or larger.
%Suppose that Xi is one-dimensional.



%\begin{equation}
%W_0(\bar{\beta} - \beta_0) = V_0 + (s.o.),
%\end{equation}


%\[  W_0 = \sum_{i=1}^{n} [g^{(1)}(X_i'\beta_0)]^2[X_i - E(X_i|X_i'\bet_0][X_i - E(X_i|X_i'\beta_0)]' \]

%and

%\[ V_0 = \sum_{i=1}^{n} u_ig^{(1)}(X_i'\beta_0)[X_i - E(X_i|X_i'\beta_0).\]

%Then, by the standard law of large numbers and a central limit theorem (CLT) argument, we have

%\begin{equation}
%\sqrt{n}(\bar{\beta} - \beta_0) = (W_0/n)^{-1}n^{-\frac{1}{2}}V_0 + o_p(1) \overset{d}{\to} N(0,\Sigma_0),
%\end{equation}

%where $\Sigma_0$ is the same as $\Sigma_I$, except that for the former $W(X_i) = 1$.


\subsection{Weight Function} % (fold)
\label{sub:Weight Function}

The weight function, $w$, is chosen so as to maximize the asymptotic efficiency of the estimator. Within the class of weighted nonlinear least squares (WNLS) estimators, an estimator is asymptotically efficient given the covariance matrix $\Omega$ of its asymptotic distributions and the covariance matrix $\Omega^*$ of any WNLS estimator, if $\Omega^* - \Omega$ is positive semidefinite. Even though the class of all regular estimators of single index models may include estimators that are not semiparametric WNLS estimators, the definition of asymptotically efficient estimator remains the same. Chamberlain (1987) investigated the case for nonlinear regression model (not necessarily a single index model) in which $g$ is known. The model is $E(Y|X = x) = g(x, \beta_0)$. The variance function, $\sigma^2(x) = E\{[Y - g(x, \beta_0)]^2| X = x\}$, is unknown. Chamberlain (1986) showed that the efficiency bound is 

\[ \Omega_{NLR} = \left\{E\left[\frac{1}{\sigma^2}\frac{\partial g(X, \beta_0)}{\partial \beta} \frac{\partial g(X, \beta_0)}{\partial \beta'}\right]\right\}^{-1}. \]

For a WNLS estimator of $\beta_0$ with weight function $w(x) = \frac{1}{\sigma^2(x)}$. The problem of efficient estimation of $\beta_0$ in a single index model with unknown $g$ has been investigated by Hall and Ichimura (1991) and Newey and Stocker (1993). Under certain regularity conditions, for $X \in A_{\delta}$ and $w(x) = \frac{1}{\sigma^2(x)}$, the efficiency bound is

\begin{equation}
\Omega_{SI} = \left\{ E\left[\frac{I(X \in A_{\delta})}{\sigma^2(x)}\frac{\partial g(X'\beta_0, \beta_0)}{\partial \hat{\beta}}\frac{\partial g(X'\beta_0, \beta_0)}{\partial \hat{\beta}'}\right] \right\}^{-1}.
\end{equation}


This bound is achieved by the semiparametric WNLS estimator if $\sigma^2(X)$ is known or independent of X. The assumption that $X \in A_{\delta}$ can be replaced by the assumption that $A_{\delta}$ grows very slowly as $n$ increases. When $\sigma^2(X)$ is unknown it can be replaced by a consistent estimator, say $s_{n}^{2}(x)$. If $w(X) = \frac{1}{s_{n}^{2}(x)}$ in the semiparametric WNLS estimator, the asymptotic efficiency bound will be reached. Thus, an asymptotic efficient estimator of $\beta_0$ is obtained even when $\sigma^2(X)$ is unknown. This consistent estimator can be obtained with a two-step procedure. First, estimate $\beta_0$ by minimizing function (9) for $w(x) = 1$. We obtain an estimator that is $n^{\frac{1}{2}}$-consistent and asymptotically normal but inefficient. The second step uses $\hat{w}_i(x) = \frac{1}{\hat{\sigma}_{i}^{2}}$, where  $\hat{\sigma}_{i}^{2}$ is defined by Robinson (1987) as 
\[\hat{\sigma}_{i}^{2} = \frac{1}{k}\sum_{j=1}^{N} \mathbf{1}{(x_j \in N_k(x_i))}\hat{\epsilon}_{j}^{2} ,\]

and $\hat{\epsilon}_i = y_i - \hat{g}(x_i'\hat{\beta_0})$, with $\hat{\beta_0}$ obtained on the first step.



\section{Klein and Spady's Binary Choice Estimator} % (fold)
\label{sec:section_about_references_within_the_document}
In this section we analyze Klein and Spady's (1993) semiparametric binary choice model, used to estimate model (2) when $Y \in \{0,1\}$.   This method exhibits $n^{-\frac{1}{2}}$ efficiency, asymptotic normality and asymptotic efficiency. The model is

\begin{equation}
Y_i =  \mathbf{1}{(X_i'\beta \geq \epsilon_i)}
\end{equation}
Assume $\epsilon_i$ is an error with distribution function $g$ and independent of $X_i$. For known $g$, the asymptotically efficient estimator of $\beta$ is a maximum likelihood estimator (MLE). The estimated $log-likelihood$ function is

\begin{equation}
\mathcal{L}(\beta) = \sum_i (1 - Y_i)ln( 1 - g(X_i'\beta)) + \sum Y_iln[g(X_i'\beta)],
\end{equation}

where

\begin{enumerate}
		\item X is a vector of exogenous variables, $\beta$ is an unknown paramenter vector and $\epsilon$ is a random disturbance;
		\item Observations $\{x_i,y_i\}$ are i.i.d..
\end{enumerate}

The function $\mathcal{L}(\beta)$ is analogous to $S_n(\beta,g)$. Similiarly to Ichimura's method, Klein and Spady suggest replacing $g$ with the leave-one-out NW from equation (8). With this in mind, we get obtain instead

\begin{equation}
\mathcal{L}_n(\beta) = \sum_i \{ (1 - Y_i)ln( 1 - \hat{g}_{-i}(X_i'\beta)) + \sum Y_iln[\hat{g}_{-i}(X_i'\beta)]\}\tau_{i},
\end{equation}

where the trimming function satisfies Proposition 4.1.:

\newtheorem{proposition}{Proposition}[section]
\begin{prop}[Trimming function for Klein and Spady's model]
With $h_N \rightarrow 0$, the trimming function employed to downweight observations has the form
\[ \tau(t \text{;}\epsilon) \equiv \{(1 + exp[h_{N}^{\frac{\epsilon}{5}} - t)/h_{N}^{\frac{\epsilon}{4}}]\}^{-1}, \]
where $\epsilon > 0$ and $t$ is to be interpreted as a density estimate.
\end{prop}

With $t$ aboved replaced by an estimated density, the function downweights observations for which the corresponding densities are small.
For a binary choice model, it is known that $Var(Y|X = x) = g(x'\beta)[1 - g(x'\beta)]$ and $\sigma^2(x)$ depends only on the single index $z = x'\beta$. One can show that %%Proof????

\[ \frac{\partial g(z,\beta)}{\partial \beta} = g^{(1)}(z)\{x - E[X|X'\beta = z, X \in A_{\delta}]\}.\]

By substituting this result in equation (11) and $\Omega_I$ in Theorem 3.1., it becomes clear that the covariance matrix of the asymptotic distribution of the semiparametric WNLS estimator of $\beta$ is the same whether the estimator of $g$ is weighted or not. Futhermore, by differentiating the right-hand side of equation (14) we get 

\[ \frac{1}{n} \sum_i \tau_{i} \frac{Y_i - \hat{g}_{-i}(X_i'\beta, \beta)}{\hat{g}_{-i}(X_i'\beta)(1 - \hat{g}_{-i}(X_i'\beta))} \frac{\partial\hat{g}_{-i}(X_i'\beta)}{\partial \beta} = 0 \]

with probability approaching 1 as $n \rightarrow \infty$. This is the same as the first-order condition for semiparametric WNLS estimaton of $\beta$ with the estimated weight function
\begin{align*}
w(x) & = \{ \hat{g}_{-i}(X_i'\beta, \beta)[ 1 - \hat{g}_{-i}(X_i'\beta, \beta)]\}^{-1} = \{ \hat{g}(X_i'\beta, \beta)[ 1 - \hat{g}(X_i'\beta, \beta)]^{-1} \\
	 & + o_p(1) = [Var(Y|X = x)]^{-1} + o_p(1)
\end{align*}

It follows that semiparametric maximum-likelihood estimator of $\beta$ in Klein and Spady's model achieves $\Omega_{NLR}$. 
Under additional assumptions such as the use of higher order kernels, Klein and Spady (1993) show that $\hat{\beta}_{KS}$ is $n^{-\frac{1}{2}}$ and has asymptotic normal distribution given by

\begin{equation}
\sqrt{n}(\hat{\beta}_{KS} - \beta) \rightarrow N(0,\Omega_{KS}),
\end{equation}
where \[ \Omega_{KS} = \left[ E \left\{ \frac{\partial P}{\partial \beta}\left(\frac{\partial P}{\partial \beta}\right)'\left[\frac{1}{P(1 - P)}\right]\right\}\right] \]

where $ P = P( \epsilon < x'\beta) = F_{\epsilon|x}(x'\beta)$, where $F_{\epsilon|x}(\cdot)$ is the CDF of $\epsilon_i$ conditional on $X_i = x$. This semiparametric estimator is therefore asymptotically as efficient as the parametric NLS estimator based on the known true functional form $g(\cdot)$.\footnote{This is only true for "first order efficiency", i.e., when $E(X_i|X_i'\beta)$ is linear in $X_i'\beta$.}
% section section_about_references_within_the_document (end)

\section{Bandwidth Selection} % (fold)
\label{sec:Bandwidth Selection}


\newpage 

\bibliographystyle{natdin}
	\bibliography{references} % expects file "references.bib"
	\addcontentsline{toc}{section}{References}
\end{document}