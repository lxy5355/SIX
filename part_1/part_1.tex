\documentclass[a4paper]{article}
\setlength{\headheight}{1.1\baselineskip}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
% package for including graphics with figure-environment
\usepackage{graphicx}
\usepackage{hyperref}
% colors for hyperlinks
% colored borders (false) colored text (true)
\hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

% package for bibliography
\usepackage[authoryear,round]{natbib}
% package for expectation signs
\usepackage{amsmath,amssymb,mathtools,bm,etoolbox}
%\documentclass[a4paper,11pt]{report} 
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{enumitem} 
\usepackage{amsmath, amsthm, amssymb}
\usepackage{amsmath}
\newcommand{\abs}[1]{ \left\lvert#1\right\rvert} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% package for header
\usepackage[automark]{scrpage2}
\pagestyle{scrheadings}
\ihead[]{Name of students}
\ohead[]{\today}
\cfoot[]{\pagemark} 
\setheadsepline[122mm]{0.3mm}

\begin{document}
	\title{
	%\begin{figure}[!ht]
	%	\flushleft
	%		\includegraphics[width=0.7\textwidth]{logo.eps}
	%\end{figure}
	\vspace{1cm}
	\Huge Here you can insert the title of \\ your seminar paper \\
	}
	
	\vspace{1cm}
	
	% if you are the only author, you might use the following
	% \author{Name of student}	
	
	% Insert here your name and correct mail address
	\author{\Large \href{mailto:first.student@smail.fh-koeln.de}{First Student} \and \Large \href{mailto:second.student@smail.fh-koeln.de}{Second Student}
	\vspace{1cm}}
	
	% name of the course and module
	\date{
	\large Module: Modulename \\ Course: Coursename \\ 
	\vspace{0.8cm}
	\large Lecturer: Name of the lecturer \\
	\vspace{1cm}
	\today
	}

	\maketitle
	\setlength{\parindent}{0pt}

\vspace{2cm}
\begin{abstract}


\end{abstract}
	\newpage
	\tableofcontents
	\newpage
	
\section{Introduction} % (fold)
\label{sec:introduction}

Semiparametric single index models are widely used in a variety of settings.
In this section, we elaborate on the model's main features and contributions to the class of semiparametric models.  
Essentially, while the risk of mispecification is reduced relative to the overly restrictive but interpretable parametric models, it also avoids inconveniences of fully nonparametric models such as the curse of dimensionality, difficulty of interpretation, and the lack of extrapolation capability.

\vspace{2mm} 
Following Li and Racine (2007) notation, the model
 
\begin{eqnarray}
Y = g(X'\beta) + \epsilon,  % with i =1,2 ..., n?, iid?
\end{eqnarray}

where

\begin{enumerate}[label=(\roman*)]
		\item $\{x_i,y_i\}$ for i = 1, ..., n is an i.i.d. sample;
		\item Y is the dependent variable, $X\in \mathbb{R}^{q}$ is a vector of explanatory variables, $\beta$ is the q $\times$ 1 vector of unknown parameters; 
	\item $X'\beta$ is a single index because it is a scalar;
	\item $ E(\epsilon|x) = 0 $;
	\item $g: \mathbb{R} \rightarrow \mathbb{R} $ is not known; % dimensions?
\end{enumerate}

is a single index model.
\vspace{2mm}

The above model is referred to as a linear single index model by Ichimura (1993). This model is semiparametric as the functional form of the linear index is specified, while $g(\cdot)$ is left unspecified and the conditional probability of $\epsilon$ conditioned on X is not specified except $ E(\epsilon|X) = 0 $. 

While Y can be of either discrete or continuous character, for illustrative purposes, we now analyze binary choice models in the semiparametric single index setting proposed by Li and Racine (2007). Thus, when considering the relationship between a binary dependent variable (Y) and covariates (X) this relationship can be modelled as follows:

\[
    Y_i = 
    \begin{cases}
      1, & \text{if}\ Y_i^* \stackrel{def}{=} \alpha + X_i'\beta + u_i > 0 \\
      0, & \text{if}\ Y_i^* = \alpha + X_i'\beta + u_i \leq 0
    \end{cases}
\]

where $Y^{*}$ is a latent variable.
Consider Y representing labor force participation, equal to 1 if the individual participates in the labor market and 0 otherwise. The explanatory variables can contain a series of economic factors that might influence labor participation, such as gender, age, marital status and education. Assuming a linear relationship between labor force participation and potential determinants, the empirical analysis focuses on the estimation of $\beta$.
Parametric methods to estimate $\beta$ require assumptions on the distribution of the error term \textit{u}. A common assumption in the parametric framework is $ u \sim N(0, 1)$. \footnote{With the identification condition $\sigma = 1$, $\beta$ can be jointly identified (Madalla (1986)) and we can use maximum likelihood to estimate it.}  Let $F_u(\cdot)$ denote the true CDF of \textit{u}. Then we get:


\[ 
\begin{split}
E(Y|x) & = \sum_{y=0,1} yP(y|x) = 1 \times P(Y=1|x) + 0 \times P(Y=0|x) \\
 & = P(Y=1|x) = P(\alpha + x'\beta + u_i > 0) \\
 & = P(u_i > -(\alpha + x'\beta)) = 1 - P(u_i \leq -(\alpha + x'\beta)) \\
 & = 1 - F(-(\alpha + x'\beta)) \equiv m(\alpha + x'\beta),
\end{split}
\]


where $F(\cdot)$ is the cummulative distribution function (CDF) of $u$. Since $u$ has a symmetric distribution and $1 - F(-(\alpha + x'\beta)) = F(\alpha + x'\beta)$. Then model (2) is commonly referred to as a Probit:

\[
E(Y|X) = P(Y=1|x) = \Phi(\alpha + x'\beta),
\]

where $\Phi$ is the CDF of a standard normal variable. Alternatively, for $u$ having a symmetric logistic distribution a logit model is obtained. Furthermore, for $g$ being the identity function we get linear model and if $g(X'\beta) = E(Y|X=x)$ model (1) represents a tobit model with $Y = max (0, g(X'\beta) + \epsilon)$, where $\epsilon$ is an unobserved, normally distributed random variable with mean zero and independent of X. 
Thus, different functional forms for $u$ lead to different functional forms for the conditional probability of $Y = 1$. Therefore, consistent parametric estimation of $E(Y|X) = P(Y=1|x)$ requires the \textit{correct} distribution specification of $u$. Generally speaking, model (1) contains many widely used parametric models that assume that $g$ is known up to a finite-dimensional parameter. 
Hence, model (1) with unknown $g(\cdot)$ leads to a more flexible version of parametric models while retaining many of its desirable features. As shown above, model (1) avoids the problem of error distribution mispecification. 

Consider now the fully nonparametric models
\[Y = g(X, \epsilon)\]
with smooth $g$ and given sampling,  or  
\[Y = g(X) + \epsilon\]
that additionally assumes the additivity of the error. These models typically suffer from the curse of dimensionality, that is, as the dimensions of the model increase, the convergence rate of the estimator decreases.
The single index model avoids the curse of dimensionality by reducing the p-dimensional predictor to a univariate single-index. Then the estimator achieves the same convergence rate $n^{-\frac{1}{2}}$ as parametric models. Thus, the single index model reaches greater estimation precision than fully nonparametric estimator with multidimensional X and the same precision as a parametric model.

Moreover, the assumptions of a single index model are weaker than those of a parametric model and stronger than those of a fully nonparametric model.  \footnote{However, the necessary assumptions for consistent parametric estimation can be relaxed. In particular, the single index model might have weaker assumptions than a fully parameterized model for structural economic models.} However, semiparametric models have two important disavantages. First, they are hard to compute. Second, they may generate multiple local minima as they require optimizing nonlinear objective functions that might be multimodal or nonconvex. \footnote{Noncave in the case of the maximum likelihood estimator.} These problems seem to be exarcebated for increasing sample sizes or number of explanatory variables (Manski, 1975, 1985; Manski and Thompson, 1986; Cosslett, 1983; Ichimura, 1993; Horowitz, 1992; and Klein and Spady, 1993).\footnote{H{\"a}rdle and Stoker (1989) suggest a direct estimation method that does not require solving an optimization problem involving iterative solutions. This method becomes less attractive than Ichimura (1993) for small-sample settings as the curse of dimensionality inherited from the first stage might not disappear on the second stage. }
% and as accurate as a one-dimensional nonparametric mean regression for $g$.

% section introduction (end)
The main focus of this paper is Ichimura's semiparametric model. In section 2 the identification conditions necessary for uniquely determining $\beta$ and $g(\cdot)$ in semiparametric models are presented.  Furthermore, Ichimura's solution will be analyzed in detail in section 3. The latter section briefly explains the weight function and bandwidth selection. As a comparison to Ichimura's method, Klein and Spady's (1993) semiparametric binary choice model will be explained concisely in section 4. Finally, in section 5 the two models are compared in both a theoretical and applied perspective.

\section{Identification} % (fold)
\label{sec:Identification}

Model (1) implies

\begin{equation}
E(Y|x) = g(x'\beta_0).
\end{equation}

Thus Y depends on $x$ only through the linear combination $x'\beta_0$, and this relationship is characterized by the link function $g(\cdot)$. Before estimating  $\beta$ and $ g(\cdot)$, restrictions must be imposed to ensure identification of the semiparametric model represented in equation (2).



\newtheorem{prop}{Proposition}[section]

\begin{prop}[Identification of a Single Index Model] 
Identification of $\beta_0$ and $g(\cdot)$ in model (1) requires:
\begin{enumerate}[label=(\roman*)]
\item x should not contain a constant and it must contain at least one continuous variable with nonzero coefficient. Furthermore, one component of $\beta_0$ is set to 1. 
\item The support of $x'\beta_0$ is bounded convex set with at least one interior point. $g$ is differentiable and it is not a constant function on the support of $x'\beta_0$;
\item For the discrete components of $x$, varying the values of the discrete variables will not divide the support of $x'\beta_0$ into disjoint subsets.
\end{enumerate}
\end{prop}

Some intuition is now provided for the restrictions imposed on $x$ in $(i)$. First, it is worth emphasizing that the elements of $x$ cannot suffer from multicollinearity, i.e., $Pr(x'\alpha = c) = 1$ where $\alpha$ is a constant and c is a scalar. Furthermore, the requirement that $x$ contains at least one continuous variable (with nonzero coefficient) prevents that $x$, as well as the scalar variable $ v = x'\beta_0 $ for any vector $\beta_0$, have a finite support. Otherwise, $E(Y|X = x) = g(x'\beta_0)$ would impose only a finite number of restrictions on $g(\cdot)$, leading to a infinite number of different choices for $g(\cdot)$ and $\beta_0$ that satisfy those restrictions.\footnote{Note however that if $g$ is assumed to be increasing, we can identify bounds on the components of $\beta_0$. See Horowitz (1998) for  concrete examples.} Similarly, \textit{location normalization} and \textit{scale normalization} requirements are necessary. Define the function $g^{*}$ by the relation $g^{*}(\gamma + v\delta) = g(v)$ for all $v$ in the support of $x'\beta_0$. Then

\begin{equation}
E(Y|X = x) = g(x'\beta_0)
\end{equation}

and

\begin{equation}
E(Y|X = x) = g^*(\gamma + x'\beta_0\delta)
\end{equation}

The models (3) and (4) are observationally equivalent. Thus, $\beta_0$ and $g$ are not identified unless restrictions are imposed to uniquely specify $\gamma$ and $\delta$. The restriction on $\gamma$ is then a \textit{location normalization} and it can be satisfied by for example requiring $x$ not to include a constant. The restriction on $\delta$ is a \textit{scale normalization}. In this paper the approach that assumes that the vector $\beta_0$ has one of its components set to 1 is used. \footnote{This implies that X must have at least 2 dimensions. Otherwise $\beta_0$ is simply normalized to 1 and a one-dimensional nonparametric model $E(y|x) = g(x)$ with no semiparametric part is obtained instead.}

As to what concerns part (ii), $g(\cdot)$ must also suffer some restrictions, even though some of these can potentially be weakened. First, $g(\cdot)$ cannot be a constant function. Otherwise $\beta_0$ is not identified. Furthermore, what makes the identification of $E(Y|X = x)$ possible is that it remains constant if $x$ changes in a way such that $x'\beta_0$ stays constant. However $P(x'\beta_0 = c)=0$, for $x_0'\beta$ continuously distributed and for some constant $c$, thus impossibilitating identification. By adding the assumption that $g$ is differentiable, $g(x'\beta_0)$ is close to $g(c)$ whenever $x'\beta_0$ is close enough to $c$. Then, the set of $x$ for which $x'\beta_0$ is within any specified nonzero distance of $c$ has nonzero probability for $c$ in the interior of the support of $x'\beta_0$. Therefore, we identify $\beta_0$ by the approximate constancy of $x'\beta_0$. In fact, Wei Lin and Kulasekera (2007) show that the weaker assumption that $g$ is only continuous is sufficient for identification. However, assuming differentiability will become useful when analyzing Ichimura's and Klein and Spady's methods.

The need for condition (iii), that is,  the need to prevent $x'\beta_0$  from being divided into disjoint subsets, can now be explained with and example inspired on Horowitz's (1998). Consider a Single Index Model in which X as a continuous component $X_1$ with support $\big[0,1\big]$, and one discrete component $X_2$ whose support is $\{0,1\}$. Assume $X_1$ and $X_2$ are independent, $g$ is strictly increasing and non periodic and set $\beta_1 = 1$ as a \textit{scale normalization}. Consider in particular the case:

\[
\begin{split}
E[Y| X = (x_1,0)]& = g(x_1), \text{support\ of } g(\cdot): [0,1];  \\
E[Y| X = (x_1,1)]& = g(x_1+\beta_2), \text{support\ of } g(\cdot): [\beta_2,1+\beta_2].
\end{split}
\]

For the case where $X_2 = 0$ the function $g$ is identified on $\big[0,1\big]$. However, for $\beta_2 > 1$ the support of $ X_1 + \beta_2$ is disjoint from $\big[0,1\big]$ and $\beta_2$ is an intercept in the model for $E(Y|(x_1,1))$. Thus, it is not possible to identify $\beta_2$. However, for $\beta_2 < 1$ the support of $X_1$ and $X_1 + \beta_2$ overlap. In particular, for $\beta_2$ positive and the subset $\{X: X_1 \in [\beta_2, 1] \wedge X_2 = 1\}$, $g(x_1 + \beta_2) = g(v)$ for some $v \in [0,1]$. \footnote{For $\beta_2$ strictly negative, one can consider $\{X: X_1 \in [ - \beta_2, 1] \wedge X_2 = 1\}$.} Then, $g(v)$ can be identified for $v \in [\beta_2, 1]$ by observations of $X_1$ for which $X_2 = 0$. Therefore, $\beta_2$ can be identified by solving

\begin{equation}
E[Y| X = (x_1,1)] = g(x_1 + \beta_2),
\end{equation}

for $x_1 \in \big[\beta_2, 1\big]$. \footnote{Note that if g was periodic on $\big[0, 1 - \beta_2 \big]$, (7) would have at least two solutions and $\beta_2$ would not be identified.}


\section{Ichimura's estimation model} % (fold)
\label{sec:Ichimura's estimation model}

In this section Ichimura's (1993) estimation method for semiparametric models is analyzed. This method exhibits $n^{-\frac{1}{2}}$ efficiency and asymptotic normality. A weighting matrix that reaches the semiparametric efficiency bound is investigated. Nonetheless, multiple local minima may result. 

Let $\beta_0$ denote the true value of $\beta$. Were $g(\cdot)$ to be known and a nonlinear least squares (NLS) method could be used to estimate $\beta_0$ by minimizing:

\begin{equation}
S_n(\beta) = \frac{1}{n}\sum_{i = 1}^n\big[Y_i - g(X_i'\beta)\big]^2
\end{equation}

with respect to $\beta$.
In this case as $g(\cdot)$ is not know we need to estimate it. However, this cannot be done directly with kernel estimation as $\beta_0$ is also unknown. Still, for a given $\beta$ we can estimate:

\begin{equation}
G(X_i'\beta) \stackrel{def}{=} E(Yi|X_i'\beta) = E[g(X_i'\beta_0)|X_i'\beta]
\end{equation}
 by the kernel method. Ichimura (1993) proposes modifications of the usual kernel estimation to estimate $G(X_i'\beta)$. Observation $i$ is excluded from the calculation of $G(X_i'\beta)$. To understand why this is necessary, suppose a relatively small bandwidth is used. Then $S_n(\beta)$ would be trivially minimized when $\hat{G}(X_i'\beta) = Y_i$. By leaving one observation out, this problem is countered. Also, this validates the ability to predict the $i$th observation using the remaining observations in the sample. Therefore, outside the sample prediction is improved. Thus a leave-one-out Nadaraya-Watson (NW) kernel estimator is obtained:

\begin{equation}
\hat{G}_{-i}(X_i'\beta) \equiv \hat{E}_{-i}(Yi|X_i'\beta) = \frac{(nh)^{-1}\sum_{j=1, j \neq i }^{n}  Y_jK(\frac{X_j'\beta - X_i'\beta}{h})}{\hat{p}_{-i}(X_i'\beta)},
\end{equation}

where $\hat{p}_{-i}(X_i'\beta) = \frac{1}{nh}\sum_{j=1,j \neq i}^{n}K(\frac{X_j'\beta - X_i'\beta}{h})$.  The denominator $\hat{p}(X_i'\beta)$ is random and it becomes necessary to trim out small values, particularly at the tails of the distribution. Let $p(x'\beta)$ denote the probability density function (PDF) of $X_i'\beta$ and $A_\delta$ and $A_n$ be the sets

\[ A_\delta = \{ x : p(x'\beta) \geq \delta, \text{ for all }  \beta \in \mathcal{B} \}
\]

where $\delta > 0$ is a constant, $\mathcal{B} \in \mathbb{R}^q$, and

\[ A_n = \{ x : \norm{x - x^*} \leq 2h_n \text{ for some } x^* \in A_\delta\}.
\]

Then for $x \in A_\delta$ the denominator does not get too close to zero. The set $A_n$ where $\norm{\cdot}$ is a Euclidean norm, is larger than $A_\delta$ but as $ n \rightarrow \infty $, $h_n \rightarrow 0$, $A_n$ shrinks to $A_\delta$. With this in mind it is possible to choose $\beta$ by using a weighted NLS (WNLS) method:

\begin{equation}
S_n(\beta) = \frac{1}{n} \sum_{i=1}^{n}  [Y_i - \hat{G}_{-i}(X_i'\beta)]^2w(x_i)\mathbf{1}{(X_i \in A_n)}
\end{equation}

where $\mathbf{1}{(Xi \in A_n)}$ is a trimming function that ensures that the random denominator is positive with high probability, and $w(x_i)$ is an appropriate nonnegative weighting function for possible heteroskedasticity explained in section 3.2. 

In what follows, certain conditions are necessary to derive the asymptotic distribution of $\hat{\beta}$. In particular, a second order kernel with bounded support must be used and $ h_n = O(n^{-\frac{1}{5}})$.

\newtheorem{theorem}{Theorem}[section]

\begin{theorem}
According to Ichimura (1993) we have

%\begin{enumerate}[label=(\roman*)]
	%\item The set $A_\delta$ is compact, and the weight function $w(\cdot)$ is bounded and positive on $A_\delta$. Define the set $D_z = \{ z : z = x'\beta, \beta \in \mathcal{B}$,$ x \in A_\delta\}$. Letting $p(\cdot)$ denote the PDF of $z \in D_z$, $p(\cdot)$ is bounded below by a positive constant for all $z \in D_z$.
	%\item $g(\cdot)$ and $p(\cdot)$ are three times differentiable with respect to $z = x'\beta$. The third derivatives are Lipschitz continuous uniformly over $\mathcal{B}$ for all $z \in D_z$.
	%\item The kernel function is a bounded second order kernel having bounded support, is twice differentiable, and its second derivative is Lipschitz continuous.
	%\item $E|Y^m| < \infty$ for some $m \geq 3$. $cov(Y|x)$ is bounded and bounded away from zero for all $x \in A_\delta$. $ln(h)/[nh^{3 + \frac{3}{m-1}}] \rightarrow 0$ and $nh^8 \rightarrow 0$ as $n \rightarrow \infty $ .
	
%\end{enumerate}


\[ \sqrt{n}(\hat{\beta}_n - \beta_0) \stackrel{d}{\rightarrow} N(0,\Omega_I), \] where $\Omega_I = V^{-1}\Sigma V^{-1}$, and

\[\Sigma = E\{w(X_i)^2\sigma^2(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0)) \times (X_i - E_A(X_i|X_i'\beta_0))'\},\]

with $g_i^{(1)} = [\partial g(v)/\partial v]|_{v = X_i'\beta_0}, E_A(X_i|v) = E(X_i|x_A'\beta_0 = v)$ with $x_A$ having the distribution of $X_i$ conditional on $Xi \in A_\delta$, and

\[ V = E[w(X_i)(g_i^{(1)})^2(X_i - E_A(X_i|X_i'\beta_0))(X_i - E_A(X_i|X_i'\beta_0))'].\]

\end{theorem}

Therefore, $\hat{\beta}_n$ has rate of convergence $n^{-\frac{1}{2}}$, that is, the same rate as for parametric models. Also, $h_n = O(n^{-\frac{1}{5}})$, which is the optimal smoothing for a  nonparametric density.
A consistent estimator of $\Omega_I$ is given by

\[ \hat{\Omega}_I = \hat{V}^{-1}\hat{\Sigma}\hat{V}^{-1}, \]

where $\hat{V} = n^{-1}\sum_{i} w(X_i)(\hat{g}^{(1)})^2(X_i'\hat{\beta}_n)(X_i - \hat{E}(X_i|X_i'\beta))(X_i - \hat{E}(X_i|X_i'\beta))', \hat{\Sigma} = n^{-1}\sum_{i} w(X_i)^2\hat{\epsilon}_{i}^{2}(\hat{g}^{(1)})^2(X_i'\hat{\beta}_n)(X_i - \hat{E}(X_i|X_i'\beta))', \hat{\epsilon}_i = Y_i - \hat{g}(X_i'\hat{\beta}_n), \hat{g}^{(1)}(X_i'\hat{\beta}_n) \linebreak
= [\partial \hat{g}_{-i}/\partial \beta]|_{\beta=\hat{\beta}_n}$, $\hat{g}_{-i}(X_i'\hat{\beta}_n)$ is defined in (10), $\hat{E}(X_i|X_i'\beta)' = \sum_{j} X_jK((X_i - X_j')'\hat{\beta})/ \sum_{j}K((X_i - X_j)'\hat{\beta}_n).$

For what follows the trimming set $A_\delta$ is ignored and $w(\cdot)$ is set to 1. Furthermore, assume $\beta_n - \beta_0 = O(n^{-\frac{1}{2}})$ and $\hat{\beta}_n - \beta_0 = O_p(n^{-\frac{1}{2}})$. Then, 


%and assume that the minimization over $\beta$ is done over the shrinking set $\mathcal{B}_n = \{ \beta : \norm{\beta - \beta_0} \geq Cn^{-1/2}$ for some constant $ C > 0$. This follows H\"ardle, Hall and Ichimura (1993). With this in mind and $ h \in \{h : C_1n^{-\frac{1}{5}} \geq h \geq C_2n^{-\frac{1}{5}}\}$ for some $C_2 > C_1 > 0$ we can establish a proof for Theorem 1. First,

\begin{align*}
S_{n}(\beta_n) & = \frac{1}{n}\sum_i \{ Y_i - \hat{G}_{-i}(X_i'\beta_n)\}^2 = \frac{1}{n}\sum_i\{Y_i - \hat{G}_{-i}(X_i'\beta_n) +  \hat{G}_{-i}(X_i'\beta_0) \\
			 & - \hat{G}_{-i}(X_i'\beta_0) \}^2 = \frac{1}{n} \sum_i \{Y_i - G(X_i'\beta_n) + o_p(1) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - \hat{G}_{-i}(X_i'\beta_0) \}^2 = \frac{1}{n}\sum_i \{ Y_i - G(X_i'\beta_n) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - g(X_i'\beta_0) + o_p(1) \}^2 = \frac{1}{n} \sum_i \{ g(X_i'\beta_0) + \epsilon_i - G(X_i'\beta_n) + \hat{G}_{-i}(X_i'\beta_0) \\
			 & - g(X_i'\beta_0) + o_p(1) \}^2 = \frac{1}{n} \sum_i \{ \epsilon_i + \hat{G}_{-i}(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta_n] \\
			 & + o_p(1) \}^2 = \frac{1}{n}\sum_i \{ g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta_n] +  \epsilon_i + o_p(1)\}^2 \\
			 & = \frac{1}{n}\sum_i \{ g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta_n] +  \epsilon_i\}^2 + o_p(1)
\end{align*}

Using a Taylor expansion:

\begin{align*}
g(X_i'\beta_0) - E[g(X_i'\beta_0)|X_i'\beta_n)] & = g(X_i'\beta_0) - g(X_i'\beta_n) \\
											 & - g^{(1)}(X_i'\beta_n)E[(\beta_0 - \beta)X_i'|X_i'\beta_n] + o_p(1) \\
				                              & = g^{(1)}(X_i'\beta_n)( X_i - E[X_i'|X_i'\beta_n)(\beta_0 - \beta_n) + o_p(1)
\end{align*}


Knowing $E(\epsilon) = 0$, the mean of $\epsilon_i$ is $o_p(1)$. Then, $\frac{1}{n}\sum_i\epsilon_io_p(1) = o_p(1)$. Hence, for $g_{i}^{(1)} = g^{(1)}(X_i'\hat{\beta}_n)$ and $v_i = X_i - E[X_i'|X_i'\hat{\beta}_n]$:

\begin{align*}
S_{n}(\hat{\beta}_n) & = (\beta_0 - \hat{\beta}_n)'\left[\frac{1}{n}\sum_i (g_i^{(1)})^2v_iv_i'\right](\beta_0 - \hat{\beta}_n) \\
             & + 2\frac{1}{n}\sum_i\epsilon_ig_i^{(1)}v_i'(\beta_0 - \hat{\beta}_n) + \frac{1}{n}\sum_i \epsilon_i^2 + o_p(1)
\end{align*}

Minimizing in order to $\hat{\beta}_n$ and ignoring terms independent of $\hat{\beta}_n$ and $o_p(1)$:
\[2\frac{1}{n}(\beta_0 - \hat{\beta}_n)\sum_i(g_{i}^{(1)})^2v_iv_i' - 2\frac{1}{n}\sum_i\epsilon_ig_i^{(1)}v_i' = 0 \]
Then, 
\begin{align*}
\sqrt{n}(\beta_0 - \hat{\beta}_n) & = (\frac{1}{n}\sum_i(g_{i}^{(1)})^2v_iv_i')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i}^{(1)}v_i \\
     					  & = (\frac{1}{n}\sum_i(g_{i0}^{(1)})^2v_{i0}v_{i0}')^{-1}\frac{1}{\sqrt{n}}\sum_i\epsilon_i g_{i0}^{(1)}v_{i0} + o_p(1).
\end{align*}

For $g_{i0}^{(1)} = g^{(1)}(X_i'\beta_0)$ and $v_{i0} = X_i - E[X_i'|X_i'\beta_0]$. With this in mind, the Central Limit Theorem (CLT) and Law of Large Numbers (LLN) we obtain the result from Theorem 3.1 if we set $w(X_i) = 1$.

It is important to however realize that the minimization of a nonlinear objective function such as equation (9) might be computationally costly. The WNLS estimator is computed by iterative methods. Start with an initial estimator $\hat{\beta}_n^{1}$ such as $\hat{\beta}_n^{1} = - \frac{1}{n}\sum_i y_i\hat{f'}(x_i)$ where $f'$ is the derivative of the density of $x_i$ and $\hat{\beta}_n^{1}$ follows the restrictions from section 2.\footnote{This estimator is a density weighted average derivative estimator such as in Stoker (1986).} Attain the kernel estimate $\hat{G}_{-i}(X_i'\hat{\beta}_n^{1})$ and thus $S_n(\hat{\beta}_n^{1})$. Perturb $\hat{\beta}_n^{1}$ to obtain $\frac{\partial S_n(\beta)}{\partial\beta} |_{\hat{\beta}_n^{1}}$ and update $\hat{\beta}_n^{2} = \hat{\beta}_n^{1} + A_n  \frac{ \partial S_n(\beta)}{\partial\beta}|_{\hat{\beta}_n^{1}}$ where $A_n$ is the size of the disturbance, and so on. This is computationally difficult, specially because there might be multiple local minima if the objective function is multimodal or nonconvex. Direct estimation methods that do not require optimization of problems involving iterative solutions have been developed to counter this problem, such as in H{\"a}rdle and Stoker (1989).


%Recall when we described identiOÌˆcation that we required the dimension of Xi to be 2 or larger.
%Suppose that Xi is one-dimensional.



%\begin{equation}
%W_0(\bar{\beta} - \beta_0) = V_0 + (s.o.),
%\end{equation}


%\[  W_0 = \sum_{i=1}^{n} [g^{(1)}(X_i'\beta_0)]^2[X_i - E(X_i|X_i'\bet_0][X_i - E(X_i|X_i'\beta_0)]' \]

%and

%\[ V_0 = \sum_{i=1}^{n} u_ig^{(1)}(X_i'\beta_0)[X_i - E(X_i|X_i'\beta_0).\]

%Then, by the standard law of large numbers and a central limit theorem (CLT) argument, we have

%\begin{equation}
%\sqrt{n}(\bar{\beta} - \beta_0) = (W_0/n)^{-1}n^{-\frac{1}{2}}V_0 + o_p(1) \overset{d}{\to} N(0,\Sigma_0),
%\end{equation}

%where $\Sigma_0$ is the same as $\Sigma_I$, except that for the former $W(X_i) = 1$.

\subsection{Bandwidth Selection} % (fold)
\label{sub:Bandwidth Selection}

Ichimura (1993) suggests that the smoothing parameter $h$ should be chosen so as to satisfy
 $ln(h_n)/[nh_n^{3 + \frac{3}{v-1}}] \rightarrow 0$ and $nh_n^8 \rightarrow 0$ as $n \rightarrow \infty $, where $v \geq 3$ is a positive integer whose specific values depend on the existence of a number of finite moments  of $Y$ along with the smoothness of the unknown function $g(\cdot)$. This allows for optimal smoothing, that is, $h_n = O(n^{-\frac{1}{5}})$. However, Ichimura (1993) gives a range of bandwidth which enables the construction of a root-$n$ consistent $\hat{\beta}_n$, but excludes the size of bandwidth that is optimal for estimating $g$. \footnote{Hall (1989) shows that two very different bandwidths may be necessary to construct good estimators of both $g$ and $\beta$.}
Alternatively, H{\"a}rdle et al. (1993) suggest an empirical way of selecting the bandwidth for optimal smoothing of both $g$ and $\beta$. This can be attained by selecting $h_n$ and $\beta$ simultaneously by minimizing

\begin{equation}
M(\beta, h_n) = \sum_i \left[ Y_i - \hat{G}_{-i}(X_i'\beta, h_n) \right]^2\mathbf{1}{(X_i \in A_\delta)},
\end{equation}

where $\hat{G}_{-i}(X_i'\beta, h_n) = \hat{G}_{-i}(X_i'\beta)$ and $A_\delta$ is the trimming set.	

\subsection{Weight Function} % (fold)
\label{sub:Weight Function}

In case heteroskedastic data is used, that is, if different errors do not have the same variance, the model must be changed as it is no longer efficient and standard errors and $t$-statistics are biased. Following the idea of Generalized Least Squares (GLS), the model must be transformed in order to achieve efficiency. Furthermore, analogously to GLS, in case the transformation depends upon unknown parameters, these need to be estimate and an analogue of Feasible GLS (FGLS) is obtained instead. On the latter case a weight function is used that assumes a general form of heroskedasticy. This weight function, $w$, is chosen so as to maximize the asymptotic efficiency of the estimator. 

%Within the class of weighted nonlinear least squares (WNLS) estimators, an estimator is asymptotically efficient given the covariance matrix $\Omega$ of its asymptotic distributions and the covariance matrix $\Omega^*$ of the other WNLS estimators, if $\Omega^* - \Omega$ is positive semidefinite. Even though the class of all regular estimators of single index models may include estimators that are not semiparametric WNLS estimators, the definition of asymptotically efficient estimator remains the same. 

%Chamberlain (1987) investigated the case for nonlinear regression model (not necessarily a single index model) in which $g$ is known. 

%The model is $E(Y|X = x) = g(x, \beta_0)$. The variance function, $\sigma^2(x) = E\{[Y - g(x, \beta_0)]^2| X = x\}$, is unknown. Chamberlain (1986) showed that the efficiency bound is 

%\[ \Omega_{NLR} = \left\{E\left[\frac{1}{\sigma^2}\frac{\partial g(X, \beta_0)}{\partial \beta} \frac{\partial g(X, \beta_0)}{\partial \beta'}\right]\right\}^{-1} \]

%for a WNLS estimator of $\beta_0$ with weight function $w(x) = \frac{1}{\sigma^2(x)}$. 

The problem of efficient estimation of $\beta_0$ in a single index model with unknown $g$ is analyzed by Hall and Ichimura (1991) and Newey and Stocker (1993). Under certain regularity conditions, the efficiency bound for the single index model with unknown $g$ and using only data for which $X \in A_{\delta}$ is $\Omega_I$ in Theorem 3.1., for $w(x) = \frac{1}{\sigma^2(x)}$.\footnote{The assumption that only data where  $X \in A_{\delta}$ is used can be relaxed by letting $A_\delta$ grow very slowly as $n$ increases} Thus, the weight function allows us to weight each observation by a factor proportional to the error variance. Moreover, observations with higher variance get a smaller weight. The efficiency bound is then

\begin{equation}
\Omega_{SI} = \left\{ E\left[\frac{1}{\sigma^2(x)}\frac{\partial}{\partial \beta}
 G(X'\beta,\beta)\frac{\partial}{\partial \beta} G(X'\beta,\beta) \right] \right\}^{-1}.
\end{equation}

%\I(X \in A_{\delta})}???

This bound is achieved by the semiparametric WNLS estimator if $\sigma^2(X)$ is known or independent of X. When $\sigma^2(X)$ is unknown the most efficient estimator is an analogue of FGLS as mentioned above.  Consider a given consistent estimator, say $s_{n}^{2}(x)$. If $w(X) = \frac{1}{s_{n}^{2}(x)}$ in the semiparametric WNLS estimator, the asymptotic efficiency bound will be reached. Thus, an asymptotic efficient estimator of $\beta_0$ is obtained even when $\sigma^2(X)$ is unknown. This consistent estimator uses a two-step procedure. First, estimate $\beta_0$ by minimizing function (9) with $w(x) = 1$. We obtain an estimator that is $n^{\frac{1}{2}}$-consistent and asymptotically normal but inefficient. The second step is a nonparametric estimation of the mean of the squared first-step residuals conditional on X. This uses the estimated weight function $\hat{w}_i(x) = \frac{1}{\hat{\sigma}_{i}^{2}}$, where  $\hat{\sigma}_{i}^{2}$ is defined by Robinson (1987) as 
\[\hat{\sigma}_{i}^{2} = \frac{1}{k}\sum_{j=1}^{n} \mathbf{1}{(x_j \in N_k(x_i))}\hat{\epsilon}_{j}^{2} ,\]
%The assumption that $X \in A_{\delta}$ can be replaced by the assumption that $A_{\delta}$ grows very slowly as $n$ increases.
and $\hat{\epsilon}_i = y_i - \hat{g}(x_i'\hat{\beta_0})$, for $\hat{\beta_0}$ obtained on the first step. Further, $N_k(x_i)$ is the set of $k$ observations of $x_j$ closest to $x_i$ in weighted Euclidean norm.


\section{Klein and Spady's Binary Choice Estimator} % (fold)
\label{sec:section_about_references_within_the_document}
In this section we analyze Klein and Spady's (1993) semiparametric binary choice model, used to estimate model (2) when $Y \in \{0,1\}$.   This method exhibits $n^{-\frac{1}{2}}$ efficiency, asymptotic normality and asymptotic efficiency. The model is defined as

\begin{equation}
Y_i =  \mathbf{1}{(X_i'\beta \geq \epsilon_i)},
\end{equation}
where $\epsilon$ is a random disturbance. Similarly to Ichimura, let $g$ denote the distribution of $\epsilon_i$. Then, $G(X_i'\beta) = E(g(X_i'\beta_0)|X_i'\beta)$. Additionaly, assume $\epsilon_i$ is independent of $X_i$. For known $g$, the asymptotically efficient estimator of $\beta_0$ is a maximum likelihood estimator (MLE). The estimated $log-likelihood$ function is

\begin{equation}
\mathcal{L}_n(\beta) = \frac{1}{n}\sum_{i=1}^n \left\{ (1 - Y_i)ln[ 1 - g(X_i'\beta)] + Y_iln[g(X_i'\beta)] \right\},
\end{equation}

where

\begin{enumerate}[label=(\roman*)]
		\item Y is binomial with realizations 1 and 0, X is a vector of exogenous variables and $\beta$ is an unknown parameter vector;
		\item $\{x_i,y_i\}$ for i = 1, ..., n is an i.i.d. sample.
\end{enumerate}

It is clear from equation (13) that restrictions must be imposed such that any estimate of $g$ is kept sufficiently far away from 0 and 1. With this in mind, Klein and Spady (1993) use a given trimming function $\tau$ employed to downweight observations for which the corresponding densities are small. However, Klein and Spady (1993) find that $\tau$ has little effect on the numerical performance of the estimator. Thus, the original trimming function can be replaced by a simplified trimming function $\tau_i = \mathbf{1}{(Xi \in A_\delta)}$, which causes little efficiency loss. The latter will be used in the remainder of this paper.

Similarly to Ichimura's method, Klein and Spady suggest replacing $g$ with the leave-one-out NW from equation (8). With this in mind, we obtain instead

\begin{equation}
\mathcal{L}_n(\beta) = \frac{1}{n}\sum_{i=1}^n \tau_{i}\{ (1 - Y_i)ln[ 1 - \hat{G}_{-i}(X_i'\beta)] +  Y_iln[\hat{G}_{-i}(X_i'\beta)]\}.
\end{equation}

In what follows, certain assumptions are required in terms of bandwidth and kernel choice. In particular, Klein and Spady (1993) use an higher order kernel. For example, by requiring
\[\int z^{2}K(z)dz = 0.\]
Furthermore, the bandwidth must satisfy the rate $ n^{-\frac{1}{6}} < h_n < n^{-\frac{1}{8}}$. Given these and other restrictions, we obtain Theorem 4.1.


\begin{theorem}
According to Klein and Spady (1993), $\hat{\beta}_{n}$ is $n^{-\frac{1}{2}}$ and has asymptotic normal distribution given by

\[\sqrt{n}(\hat{\beta}_{n} - \beta_0) \stackrel{d}{\rightarrow} N(0,\Omega_{KS}),
\]
where \[ \Omega_{KS} = \left\{ E\left[\frac{\partial}{\partial \beta}
 G(X_i'\beta)\frac{\partial}{\partial \beta} G(X_i'\beta)'\frac{1}{g(X_i'\beta_0)(1 - g(X_i'\beta_0))} \right]\right\}^{-1} \]
 
and $\Omega_{KS} = \Omega_{SI}$, i.e., the estimator is asymptotically efficient.

\end{theorem}

It now becomes necessary to elaborate on the last stament of Theorem 4.1. For a binary choice model, $Var(Y|X = x) = P(Y = 1|X = x)[1 - P(Y = 1|X = x)] = E[\mathbf{1}{(x'\beta_0 \geq \epsilon)}]\{1 - E[\mathbf{1}{(x'\beta_0 \geq \epsilon)}]\} = g(x'\beta_0)[1 - g(x'\beta_0)]$ and $\sigma^2(x)$ depends only on the single index $x'\beta_0$. Assume $\beta_n - \beta_0 = O(n^{-\frac{1}{2}})$, $\hat{\beta}_n - \beta_0 = O_p(n^{-\frac{1}{2}})$ and consider the result shown above
\begin{align*}
g(X_i'\beta_0) - E_A[g(X_i'\beta_0)|X_i'\beta)] & = g^{(1)}(X_i'\beta)( X_i - E_A[X_i'|X_i'\beta])(\beta_0 - \beta) \\
											   & + O(n^{-1})
\end{align*}
where $E_A(X_i|v) = E(X_i|x_A'\beta_0 = v)$ with $x_A$ having the distribution of $X_i$. Minimizing in order to $\beta$ and ignoring terms independent of $\beta$ and $O(n^{-1})$:
\begin{align*}
 \frac{\partial}{\partial \beta} G(X_i'\beta) & = g^{(1)}(X_i'\beta)( X_i - E_A[X_i'|X_i'\beta]) \\
 										   & = g^{(1)}(X_i'\beta_0)( X_i - E_A[X_i'|X_i'\beta]) + o_p(1).
\end{align*}

By substituting this result in equation (11) and $\Omega_I$ in Theorem 3.1., it can be seen that the covariance matrix of the asymptotic distribution of the semiparametric WNLS estimator of $\beta_0$ is the same whether the estimator of $g$ is weighted or not. Thus, the asymptotically efficient bound $\Omega_{SI}$ is reached without using a weight function. 

%\[S_n(\beta) = -\frac{1}{2n}[ y - g]'\hat{\Sigma}^{-1}[Y - g], \]

%where $\Sigma$ is the error variance matrix. Consider the model for heteroskedasticty $\sigma_i^2 = E[u_i^2|x_i] = exp(z_i'\gamma_0)$, where $z$ is a specified function of $x$. Then $\Sigma = Diag[exp(z_i'\gamma_0)]$ and $\hat{\Sigma} = Diag[exp(z_i'\hat{\gamma})]$, where $\hat{\gamma}$ can be obtained by nonlinear regression of squares NLS residuals $(y_i - g(x_i,\hat{\beta_{NLS}}))^2$ on $exp(z_i'\gamma_0)$. For diagonal $\Sigma$, $\Sigma^{-1} = Diag\left[\frac{1}{\sigma_i^2}\right]$. Then the objective function for the semiparametric WNLS simplifies to

%\[S_n(\beta) = - \frac{1}{2n}\sum_{i-1}^{n}\frac{(y_i - \hat{G}_{-i}(X_i,\beta))^2}{\hat{\sigma_i^2}}.\]

This can be more easily understood by considering a semiparametric WNLS estimation of $\beta_0$ such as with Ichimura's method. First, replace $\beta$ by $ \hat{\beta}_n$ in equation (14) and differentiate its right-hand side with respect to $\hat{\beta}_n$. Then, we obtain
\begin{equation}
\frac{1}{n} \sum_{i=1}^n \frac{\frac{\partial\hat{G}_{-i}(X_i'\beta_n)}{\partial \beta_n}}{\hat{G}_{-i}(X_i'\beta_n)(1 - \hat{G}_{-i}(X_i'\beta_n))} (Y_i - \hat{G}_{-i}(X_i'\beta_n))\tau_i X_i  = 0
\end{equation} 

with probability approaching 1 as $n \rightarrow \infty$. This is the same as the first-order condition for semiparametric WNLS estimation of $\beta_0$ with the weight function

\begin{align*}
w_i & = \{ \hat{G}_{-i}(X_i'\beta_n)[ 1 - \hat{G}_{-i}(X_i'\beta_n)]\}^{-1} = \{ G(X_i'\beta_n)[ 1 - G(X_i'\beta_n)]\}^{-1} + o_p(1) \\
     & = \{ G(X_i'\beta_0)[ 1 - G(X_i'\beta_0)]\}^{-1} + o_p(1).
\end{align*}


Considering section 3.2., it follows that semiparametric maximum-likelihood estimator of $\beta_0$ in Klein and Spady's model is asymptotically efficient.\footnote{This is only true for "first order efficiency", i.e., when $E(X_i|X_i'\beta)$ is linear in $X_i'\beta$.}

As with Ichimura's model, optimization of the maximum likelihood function using iterative methods might become difficult.  Particularly because optimization could lead to multiple local minima if the objective function is multimodal or convex. \footnote{Zhou and Lang (1995) provide an alternative using an ``easy to compute'' semiparametric estimator.}
% section section_about_references_within_the_document (end)

\subsection{Bandwidth Selection} % (fold)
\label{sub:Bandwidth Selection}

Klein and Spady (1993) do not discuss how to choose the bandwidth, apart from the requirement that $ n^{-\frac{1}{6}} < h_n < n^{-\frac{1}{8}}$. However, similarly to Ichimura's method and following H{\"a}rdle et al. (1993), an empirical way of selecting the bandwidth for optimal smoothing of both $g$ and $\beta$ can be conjectured. This is attained by choosing $h_n$ jointly with $\beta$ when minimizing
\begin{equation}
M_n(\beta, h_n) = \frac{1}{n}\sum_{i=1}^n \tau_{i}\{ (1 - Y_i)ln[ 1 - \hat{G}_{-i}(X_i'\beta, h_n)] +  Y_iln[\hat{G}_{-i}(X_i'\beta, h_n)]\}.
\end{equation}


\section{Comparing Ichimura's and Klein and Spady's model} % (fold)
\label{sec:Comparing Ichimura's and Klein and Spady's model}

Klein and Spady's model seems more approriate than Ichimura's for the binary choice model case. Ichimura's model uses a weight function as described in section 3.2. to correct for heteroskedasticity. However, Klein and Spady's model is fully efficient in the sense that it reaches the semiparametric efficiency bound.  The $log-likelihood$ function has ``natural'' weighting, thus avoiding the problem of heteroskedasticity and the use of a weight function.  Consequently, a model is attained that avoids the use of the two-step procedure described in section 3.2. to reach the asymptotic efficiency bound. 

\newpage 

\bibliographystyle{natdin}
	\bibliography{references} % expects file "references.bib"
	\addcontentsline{toc}{section}{References}
\end{document}